import os
import time
import pandas as pd
import json

from glob import glob
from tinydb import TinyDB, Query

from prism_monitor.data.database import PrismCoreDataBase
from prism_monitor.modules.event.event_detect import detect_anomalies_realtime
from prism_monitor.modules.event_precursor.precursor import precursor
from prism_monitor.modules.explanation.explanation import event_explain, event_cause_candidates
from prism_monitor.modules.risk_assessment.assessment import risk_assessment


def monitoring_event_output(status="complete", anomaly_detected=True, description="ë¼ì¸2-5 ì˜¨ë„ ì´ìƒ ê°ì§€"):
    res = {
        "isSuccess": True,
        "code": 201,
        "message": "ê²°ê³¼ ì „ë‹¬ ì™„ë£Œ"
    }
    return res

# ðŸ†• NEW VERSION: CSV íŒŒì¼ë³„ ëª¨ë¸ ì§€ì› (query_decompose í†µí•© ì¤€ë¹„)
def monitoring_event_detect(monitor_db: TinyDB, prism_core_db, start: str, end: str, task_id: str,
                           target_file: str = None, target_process: str = None, user_query: str = None):
    """
    ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ ê°ì§€ í•¨ìˆ˜

    Args:
        monitor_db: ëª¨ë‹ˆí„° DB
        prism_core_db: PRISM Core DB
        start: ì‹œìž‘ ì‹œê°„ (ISO format)
        end: ì¢…ë£Œ ì‹œê°„ (ISO format)
        task_id: íƒœìŠ¤í¬ ID
        target_process: íƒ€ê²Ÿ ê³µì • (ì˜ˆ: 'semi_cmp_sensors') - ì§ì ‘ ì§€ì • ì‹œ ì‚¬ìš©
        user_query: ì‚¬ìš©ìž ì¿¼ë¦¬ - query_decomposeë¡œ ê³µì • ìžë™ íŒë³„ ì‹œ ì‚¬ìš©

    Returns:
        ì´ë²¤íŠ¸ ê°ì§€ ê²°ê³¼
    """
    classified_process = None

    # 1. user_queryê°€ ìžˆë‹¤ë©´ query_decomposeë¡œ ê³µì • ìžë™ íŒë³„ (í–¥í›„ ê¸°ëŠ¥)
    if user_query and not target_process:
        try:
            # âš ï¸ query_decomposeê°€ classified_classë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •ëœ í›„ í™œì„±í™”
            # from prism_monitor.modules.query_decompose.query_decompose import query_decompose
            # timestamp_min, timestamp_max, result_df, classified_class = query_decompose(user_query)
            # classified_process = classified_class
            # print(f"query_decompose identified process: {classified_process}")
            print("query_decompose integration pending (another developer's task)")
        except Exception as e:
            print(f"query_decompose failed: {e}, using target_process or legacy mode")

    # 2. target_file ìš°ì„ ìˆœìœ„: ì§ì ‘ ì§€ì • > query_decompose ê²°ê³¼ > target_process(í•˜ìœ„ í˜¸í™˜)
    final_target_file = target_file or classified_process or target_process

    # 3. ì´ìƒ ê°ì§€ ìˆ˜í–‰ (íŒŒì¼ë³„ ëª¨ë¸ ì‚¬ìš©)
    anomalies, drift_results, analysis, vis_json = detect_anomalies_realtime(
        prism_core_db,
        start=start,
        end=end,
        target_file=final_target_file  # ðŸ†• íŒŒì¼ ì§€ì •
    )

    event_record = {
        "task_id": task_id,
        "records": analysis,
        "target_file": final_target_file,  # ðŸ†• íŒŒì¼ ì •ë³´ ì €ìž¥
        "validation": {
            "anomalies": anomalies,
            "drift_results": drift_results,
        }
    }
    print('analysis=', analysis)

    Event = Query()
    monitor_db.table('EventDetectHistory').upsert(
        {
            'task_id': task_id,
            'records': analysis,
            'target_file': final_target_file,  # ðŸ†• íŒŒì¼ ì •ë³´ DB ì €ìž¥
            'target_process': final_target_file  # ðŸ†• ê³µì • ì •ë³´ (explanationì—ì„œ ì‚¬ìš©)
        },
        Event.task_id == task_id
    )
    print(event_record)

    return {
        'result': {
            'status': 'complete',
            'anomalies': True if len(anomalies) else False,
            'drift_detected': True if len(drift_results) else False,
            'target_file': final_target_file  # ðŸ†• ê²°ê³¼ì— íŒŒì¼ ì •ë³´ í¬í•¨
        }
    }

# ðŸ“ OLD VERSION (ì£¼ì„ ì²˜ë¦¬ - ì°¸ê³ ìš©)
# def monitoring_event_detect(monitor_db: TinyDB, prism_core_db, start: str, end: str, task_id: str):
#     """ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ ê°ì§€ í•¨ìˆ˜"""
#     # detect_anomalies_realtimeê°€ ì´ì œ 5ê°œ ê°’ì„ ë°˜í™˜ (drift_svg ì¶”ê°€)
#     anomalies, drift_results, analysis, vis_json = detect_anomalies_realtime(prism_core_db, start=start, end=end)
#
#     event_record = {
#         "task_id": task_id,
#         "records": analysis,
#         "validation": {
#             "anomalies": anomalies,
#             "drift_results": drift_results,
#         }
#     }
#     print('analysis=', analysis)
#
#     Event = Query()
#     monitor_db.table('EventDetectHistory').upsert({'task_id':task_id, 'records':analysis}, Event.task_id == task_id)
#     print(event_record)
#     return {
#         'result': {
#             'status': 'complete',
#             'anomalies': True if len(anomalies) else False,
#             'drift_detected': True if len(drift_results) else False,  # ë“œë¦¬í”„íŠ¸ ê°ì§€ ì—¬ë¶€ ì¶”ê°€
#         }
#     }


# ðŸ†• NEW VERSION: ê³µì •ë³„ í”„ë¡¬í”„íŠ¸ ì§€ì›
def monitoring_event_explain(llm_url, monitor_db: TinyDB, task_id: str):
    # task_idì— í•´ë‹¹í•˜ëŠ” ì´ë²¤íŠ¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
    Event = Query()
    event_record = monitor_db.table('EventDetectHistory').get(Event.task_id == task_id)

    if not event_record:
        return {'error': f'No record found for task_id: {task_id}'}

    # 'records' í‚¤ì— ë‹´ê¸´ ë¶„ì„ ë°ì´í„° ì „ë‹¬
    event_detect_analysis = event_record.get('records', [])
    target_process = event_record.get('target_process')  # ðŸ†• ê³µì • ì •ë³´ ì¡°íšŒ

    explain = event_explain(
        llm_url=llm_url,
        event_detect_analysis=event_detect_analysis,
        process_type=target_process  # ðŸ†• ê³µì •ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    )
    res = {
        'explain': explain,
        'target_process': target_process  # ðŸ†• ê³µì • ì •ë³´ í¬í•¨
    }
    Event = Query()
    monitor_db.table('EventExplainHistory').upsert(res, Event.task_id == task_id)

    return res

# ðŸ“ OLD VERSION (ì£¼ì„ ì²˜ë¦¬ - ì°¸ê³ ìš©)
# def monitoring_event_explain(llm_url, monitor_db: TinyDB, task_id: str):
#     # task_idì— í•´ë‹¹í•˜ëŠ” ì´ë²¤íŠ¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
#     Event = Query()
#     event_record = monitor_db.table('EventDetectHistory').get(Event.task_id == task_id)
#
#     if not event_record:
#         return {'error': f'No record found for task_id: {task_id}'}
#
#     # 'records' í‚¤ì— ë‹´ê¸´ ë¶„ì„ ë°ì´í„° ì „ë‹¬
#     event_detect_analysis = event_record.get('records', [])
#
#     explain = event_explain(
#         llm_url=llm_url,
#         event_detect_analysis=event_detect_analysis
#     )
#     res = {
#         'explain': explain
#     }
#     Event = Query()
#     monitor_db.table('EventExplainHistory').upsert(res, Event.task_id == task_id)
#
#     return res


# ðŸ†• NEW VERSION: ê³µì •ë³„ í”„ë¡¬í”„íŠ¸ ì§€ì›
def monitoring_event_cause_candidates(llm_url, monitor_db: TinyDB, task_id: str):
    # task_idì— í•´ë‹¹í•˜ëŠ” ì´ë²¤íŠ¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
    Event = Query()
    event_record = monitor_db.table('EventDetectHistory').get(Event.task_id == task_id)

    if not event_record:
        return {'error': f'No record found for task_id: {task_id}'}

    # 'records' í‚¤ì— ë‹´ê¸´ ë¶„ì„ ë°ì´í„° ì „ë‹¬
    event_detect_analysis = event_record.get('records', [])
    target_process = event_record.get('target_process')  # ðŸ†• ê³µì • ì •ë³´ ì¡°íšŒ

    cause_candidates = event_cause_candidates(
        llm_url=llm_url,
        event_detect_analysis=event_detect_analysis,
        process_type=target_process  # ðŸ†• ê³µì •ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    )
    res = {
        "causeCandidates": cause_candidates,
        "target_process": target_process  # ðŸ†• ê³µì • ì •ë³´ í¬í•¨
    }
    Event = Query()
    monitor_db.table('EventCauseCandidatesHistory').upsert(res, Event.task_id == task_id)

    return res

# ðŸ“ OLD VERSION (ì£¼ì„ ì²˜ë¦¬ - ì°¸ê³ ìš©)
# def monitoring_event_cause_candidates(llm_url, monitor_db: TinyDB, task_id: str):
#     # task_idì— í•´ë‹¹í•˜ëŠ” ì´ë²¤íŠ¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
#     Event = Query()
#     event_record = monitor_db.table('EventDetectHistory').get(Event.task_id == task_id)
#
#     if not event_record:
#         return {'error': f'No record found for task_id: {task_id}'}
#
#     # 'records' í‚¤ì— ë‹´ê¸´ ë¶„ì„ ë°ì´í„° ì „ë‹¬
#     event_detect_analysis = event_record.get('records', [])
#
#     cause_candidates = event_cause_candidates(
#         llm_url=llm_url,
#         event_detect_analysis=event_detect_analysis
#     )
#     res = {
#         "causeCandidates": cause_candidates
#     }
#     Event = Query()
#     monitor_db.table('EventCauseCandidatesHistory').upsert(res, Event.task_id == task_id)
#
#     return res

def monitoring_event_precursor(monitor_db: TinyDB, prism_core_db: PrismCoreDataBase, start: str, end: str, task_id: str):
    start_time = pd.to_datetime(start, utc=True)
    end_time = pd.to_datetime(end, utc=True)

    datasets = {}
    try:
        raise ValueError('use local data')
        for table_name in prism_core_db.get_tables():
            df = prism_core_db.get_table_data(table_name)
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
            datasets[table_name] = df
    except Exception as e:
        print(f"dataset error raised {e}, use local data")
        data_paths = glob('prism_monitor/data/Industrial_DB_sample/*.csv')
        for data_path in data_paths:
            df = pd.read_csv(data_path)
            table_name = os.path.basename(data_path).split('.csv')[0].lower()
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
            datasets[table_name] = df

    res = precursor(datasets)
    Event = Query()
    monitor_db.table('EventPrecursorHistory').upsert(res, Event.task_id == task_id)

    return res


def monitoring_event_evaluate_risk(llm_url, monitor_db: TinyDB, task_id, topk=5):
    Event = Query()

    # 1. í˜„ìž¬ task_idì— í•´ë‹¹í•˜ëŠ” ë¶„ì„ ê²°ê³¼
    event_detect_analysis = monitor_db.table('EventDetectHistory').get(Event.task_id == task_id)

    # 2. ê³¼ê±° task_id ì¤‘ í˜„ìž¬ task_idê°€ ì•„ë‹Œ ê²ƒë§Œ ìƒìœ„ topkê°œ
    event_detect_all = monitor_db.table('EventDetectHistory').all()
    event_detect_analysis_history = [
        r for r in event_detect_all if r.get('task_id') != task_id
    ][:topk]  # ì •ë ¬ ê¸°ì¤€ í•„ìš” ì‹œ ì¶”ê°€

    # 3. í˜„ìž¬ task_idì˜ ì›ì¸ í›„ë³´ ë°ì´í„°
    task_instructions = monitor_db.table('EventCauseCandidatesHistory').get(Event.task_id == task_id)

    # 4. ê³¼ê±° task_idì˜ ì›ì¸ í›„ë³´ ë°ì´í„°
    cause_all = monitor_db.table('EventCauseCandidatesHistory').all()
    task_instructions_history = [
        r for r in cause_all if r.get('task_id') != task_id
    ][:topk]

    # ìœ„í—˜ í‰ê°€ ìˆ˜í–‰
    event_evaluation, prediction_evaluation = risk_assessment(
        llm_url=llm_url,
        event_detect_analysis=event_detect_analysis,
        event_detect_analysis_history=event_detect_analysis_history,
        task_instructions=task_instructions,
        task_instructions_history=task_instructions_history
    )


    return {
        'eventEvaluation':event_evaluation,
        'predictionEvaluation':prediction_evaluation,
    }



def monitoring_dashboard_update(field: str = "line_id", type: str = "LINE", status: str = "ë¹„ì •ìƒ", anomaly_detected: bool = True, anomaly_type: str = "temperature_spike", updated_at: str = "2025-07-17T12:01:03Z"):
    return {
        "isSuccess": True,
        "code": 200,
        "message": "ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ"
    }

# ì•„ëž˜ ì½”ë“œê°€ ì›ëž˜ ì½”ë“œ ì´ ë¶€ë¶„ ì£¼ì„ ì§€ìš°ë©´ ë¨
# def monitoring_real_time(prism_core_db):
#     """ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ ê°ì§€ í•¨ìˆ˜"""
#     # detect_anomalies_realtimeê°€ ì´ì œ 5ê°œ ê°’ì„ ë°˜í™˜ (drift_svg ì¶”ê°€)
#     end = time.now()
#     start = time.now() - pd.Timedelta(minutes=10)
#     anomalies, drift_results, analysis, vis_json = detect_anomalies_realtime(prism_core_db, start=start, end=end)
#     result = vis_json

#     return {
#         'result': result
#     }

def monitoring_real_time(prism_core_db):
    """ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ ê°ì§€ í•¨ìˆ˜"""
    end = pd.Timestamp.now()
    start = end - pd.Timedelta(minutes=10)
    
    anomalies, drift_results, analysis, vis_json = detect_anomalies_realtime(
        prism_core_db,
        start=start.isoformat(),  # ë¬¸ìžì—´ë¡œ ë³€í™˜
        end=end.isoformat()       # ë¬¸ìžì—´ë¡œ ë³€í™˜
    )
    print(vis_json)
    
    return {'visJson': vis_json}  # vis_json ì§ì ‘ ë°˜í™˜