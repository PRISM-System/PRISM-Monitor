{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d50a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 05:43:45.522995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/minjoo/.conda/envs/agi/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760dbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SemiconductorRealDataDetector:\n",
    "    \"\"\"\n",
    "    실제 반도체 공정 데이터를 활용한 이상탐지 딥러닝 모듈\n",
    "    \n",
    "    다중 테이블 데이터를 통합하여 이상탐지 수행:\n",
    "    1. LOT 관리 데이터와 센서 데이터 통합\n",
    "    2. Autoencoder + LSTM Autoencoder 앙상블\n",
    "    3. 시간 구간별 이상탐지\n",
    "    4. 공정별/장비별 세부 분석\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_base_path, config=None):\n",
    "        self.data_base_path = data_base_path\n",
    "        self.config = config or self._default_config()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.history = {}\n",
    "        self.data_files = {\n",
    "            'lot_manage': 'SEMI_LOT_MANAGE.csv',\n",
    "            'process_history': 'SEMI_PROCESS_HISTORY.csv', \n",
    "            'param_measure': 'SEMI_PARAM_MEASURE.csv',\n",
    "            'equipment_sensor': 'SEMI_EQUIPMENT_SENSOR.csv',\n",
    "            'alert_config': 'SEMI_SENSOR_ALERT_CONFIG.csv',\n",
    "            'photo_sensors': 'SEMI_PHOTO_SENSORS.csv',\n",
    "            'etch_sensors': 'SEMI_ETCH_SENSORS.csv',\n",
    "            'cvd_sensors': 'SEMI_CVD_SENSORS.csv',\n",
    "            'implant_sensors': 'SEMI_IMPLANT_SENSORS.csv',\n",
    "            'cmp_sensors': 'SEMI_CMP_SENSORS.csv'\n",
    "        }\n",
    "        \n",
    "    def _default_config(self):\n",
    "        \"\"\"기본 설정\"\"\"\n",
    "        return {\n",
    "            'sequence_length': 60,  # 시계열 길이\n",
    "            'contamination': 0.05,  # 이상치 비율 (5%)\n",
    "            'threshold_percentile': 95,  # 임계값 백분위수\n",
    "            'batch_size': 32,\n",
    "            'epochs': 50,  # 실제 데이터용으로 줄임\n",
    "            'validation_split': 0.2,\n",
    "        }\n",
    "\n",
    "    def load_and_explore_data(self):\n",
    "        \"\"\"\n",
    "        실제 데이터 로딩 및 탐색\n",
    "        \"\"\"\n",
    "        print(\"실제 반도체 데이터 로딩 및 탐색 중...\")\n",
    "        \n",
    "        # 모든 데이터 파일 로딩\n",
    "        datasets = {}\n",
    "        for key, filename in self.data_files.items():\n",
    "            file_path = os.path.join(self.data_base_path, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"로딩 중: {filename}\")\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    datasets[key] = df\n",
    "                    print(f\"  - 크기: {df.shape}\")\n",
    "                    print(f\"  - 컬럼: {list(df.columns)}\")\n",
    "                    print()\n",
    "                except Exception as e:\n",
    "                    print(f\"  - 오류: {e}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {file_path}\")\n",
    "        \n",
    "        self.raw_datasets = datasets\n",
    "        return datasets\n",
    "\n",
    "    def integrate_sensor_data(self, datasets):\n",
    "        \"\"\"\n",
    "        여러 센서 테이블을 통합하여 하나의 센서 데이터셋 생성\n",
    "        \"\"\"\n",
    "        print(\"센서 데이터 통합 중...\")\n",
    "        \n",
    "        sensor_tables = ['photo_sensors', 'etch_sensors', 'cvd_sensors', \n",
    "                        'implant_sensors', 'cmp_sensors']\n",
    "        \n",
    "        integrated_sensors = []\n",
    "        \n",
    "        for table_name in sensor_tables:\n",
    "            if table_name in datasets:\n",
    "                df = datasets[table_name].copy()\n",
    "                \n",
    "                # 공통 컬럼들만 선택 (PNO, EQUIPMENT_ID, LOT_NO, TIMESTAMP)\n",
    "                common_cols = ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'TIMESTAMP']\n",
    "                \n",
    "                # 실제 존재하는 컬럼만 선택\n",
    "                available_common = [col for col in common_cols if col in df.columns]\n",
    "                \n",
    "                if available_common:\n",
    "                    # 수치형 센서 컬럼들 찾기\n",
    "                    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                    # PNO 제외 (ID라서)\n",
    "                    sensor_cols = [col for col in numeric_cols if col != 'PNO']\n",
    "                    \n",
    "                    # 테이블 정보 추가\n",
    "                    df['SENSOR_TABLE'] = table_name.replace('_sensors', '').upper()\n",
    "                    \n",
    "                    # 센서값들을 하나의 컬럼으로 변환 (Long format)\n",
    "                    if sensor_cols:\n",
    "                        df_long = df.melt(\n",
    "                            id_vars=available_common + ['SENSOR_TABLE'],\n",
    "                            value_vars=sensor_cols,\n",
    "                            var_name='SENSOR_TYPE',\n",
    "                            value_name='SENSOR_VALUE'\n",
    "                        )\n",
    "                        integrated_sensors.append(df_long)\n",
    "                        print(f\"  - {table_name}: {len(sensor_cols)}개 센서, {len(df)}개 레코드\")\n",
    "        \n",
    "        if integrated_sensors:\n",
    "            result = pd.concat(integrated_sensors, ignore_index=True)\n",
    "            print(f\"통합 완료: 총 {len(result)}개 센서 레코드\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"통합할 센서 데이터가 없습니다.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def create_unified_dataset(self, datasets):\n",
    "        \"\"\"\n",
    "        모든 테이블을 통합하여 분석용 데이터셋 생성\n",
    "        \"\"\"\n",
    "        print(\"통합 데이터셋 생성 중...\")\n",
    "        \n",
    "        # 1. 센서 데이터 통합\n",
    "        integrated_sensors = self.integrate_sensor_data(datasets)\n",
    "        \n",
    "        # 2. LOT 관리 데이터 기준으로 통합\n",
    "        if 'lot_manage' in datasets:\n",
    "            main_df = datasets['lot_manage'].copy()\n",
    "            print(f\"기본 LOT 데이터: {len(main_df)}개 LOT\")\n",
    "        else:\n",
    "            print(\"LOT 관리 데이터가 없습니다.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # 3. 각 LOT별 센서 통계 생성\n",
    "        if not integrated_sensors.empty and 'LOT_NO' in integrated_sensors.columns:\n",
    "            # LOT별 센서 통계 계산\n",
    "            sensor_stats = integrated_sensors.groupby(['LOT_NO', 'SENSOR_TYPE'])['SENSOR_VALUE'].agg([\n",
    "                'mean', 'std', 'min', 'max', 'count'\n",
    "            ]).reset_index()\n",
    "            \n",
    "            # Wide format으로 변환\n",
    "            sensor_features = sensor_stats.pivot_table(\n",
    "                index='LOT_NO',\n",
    "                columns='SENSOR_TYPE',\n",
    "                values=['mean', 'std', 'min', 'max'],\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            # 컬럼명 정리\n",
    "            sensor_features.columns = [f\"{stat}_{sensor}\" for stat, sensor in sensor_features.columns]\n",
    "            sensor_features = sensor_features.reset_index()\n",
    "            \n",
    "            # LOT 데이터와 조인\n",
    "            main_df = main_df.merge(sensor_features, on='LOT_NO', how='left')\n",
    "            print(f\"센서 특성 추가 완료: {sensor_features.shape[1]-1}개 특성\")\n",
    "        \n",
    "        # 4. 공정 이력 데이터 통합\n",
    "        if 'process_history' in datasets:\n",
    "            process_df = datasets['process_history']\n",
    "            if 'LOT_NO' in process_df.columns:\n",
    "                # LOT별 공정 통계\n",
    "                process_stats = process_df.groupby('LOT_NO').agg({\n",
    "                    'IN_QTY': ['mean', 'sum'],\n",
    "                    'OUT_QTY': ['mean', 'sum'],\n",
    "                }).reset_index()\n",
    "                \n",
    "                process_stats.columns = [f\"process_{col[0]}_{col[1]}\" if col[1] else col[0] \n",
    "                                       for col in process_stats.columns]\n",
    "                process_stats.columns = [col.replace('process_LOT_NO_', 'LOT_NO') for col in process_stats.columns]\n",
    "                \n",
    "                main_df = main_df.merge(process_stats, on='LOT_NO', how='left')\n",
    "                print(f\"공정 이력 특성 추가 완료\")\n",
    "        \n",
    "        # 5. 파라미터 측정 데이터 통합\n",
    "        if 'param_measure' in datasets:\n",
    "            param_df = datasets['param_measure']\n",
    "            if 'LOT_NO' in param_df.columns:\n",
    "                # LOT별 파라미터 통계\n",
    "                param_stats = param_df.groupby('LOT_NO')['MEASURED_VAL'].agg([\n",
    "                    'mean', 'std', 'min', 'max'\n",
    "                ]).reset_index()\n",
    "                \n",
    "                param_stats.columns = [f\"param_{col}\" if col != 'LOT_NO' else col \n",
    "                                     for col in param_stats.columns]\n",
    "                \n",
    "                main_df = main_df.merge(param_stats, on='LOT_NO', how='left')\n",
    "                print(f\"파라미터 측정 특성 추가 완료\")\n",
    "        \n",
    "        print(f\"최종 통합 데이터셋: {main_df.shape}\")\n",
    "        return main_df\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        특성 준비 및 전처리\n",
    "        \"\"\"\n",
    "        print(\"특성 준비 및 전처리 중...\")\n",
    "        \n",
    "        # 수치형 컬럼만 선택\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # ID 관련 컬럼 제외\n",
    "        exclude_cols = ['PNO']  # 기본 제외\n",
    "        if 'FINAL_YIELD' in numeric_cols:\n",
    "            exclude_cols.append('FINAL_YIELD')  # 타겟 변수는 별도 처리\n",
    "            \n",
    "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        # 결측치 처리\n",
    "        df_processed = df.copy()\n",
    "        df_processed[feature_cols] = df_processed[feature_cols].fillna(0)\n",
    "        \n",
    "        # 이상치 라벨 생성 (수율 기반)\n",
    "        if 'FINAL_YIELD' in df.columns:\n",
    "            # 수율이 낮은 LOT을 이상으로 간주\n",
    "            yield_threshold = df['FINAL_YIELD'].quantile(0.1)  # 하위 10%\n",
    "            df_processed['is_anomaly'] = df_processed['FINAL_YIELD'] < yield_threshold\n",
    "        else:\n",
    "            # 수율 정보가 없으면 통계적 방법 사용\n",
    "            feature_data = df_processed[feature_cols]\n",
    "            z_scores = np.abs((feature_data - feature_data.mean()) / feature_data.std()).mean(axis=1)\n",
    "            threshold = np.percentile(z_scores, 90)\n",
    "            df_processed['is_anomaly'] = z_scores > threshold\n",
    "        \n",
    "        # 특성 스케일링\n",
    "        df_processed[feature_cols] = self.scaler.fit_transform(df_processed[feature_cols])\n",
    "        \n",
    "        print(f\"전처리 완료: {len(feature_cols)}개 특성\")\n",
    "        print(f\"이상 LOT 비율: {df_processed['is_anomaly'].mean():.2%}\")\n",
    "        \n",
    "        return df_processed, feature_cols\n",
    "\n",
    "    def build_autoencoder(self, input_dim):\n",
    "        \"\"\"\n",
    "        Autoencoder 모델 구축\n",
    "        \"\"\"\n",
    "        input_layer = layers.Input(shape=(input_dim,))\n",
    "        \n",
    "        # 인코더 - 차원 축소\n",
    "        encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "        encoded = layers.BatchNormalization()(encoded)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        \n",
    "        encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        encoded = layers.BatchNormalization()(encoded)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        \n",
    "        encoded = layers.Dense(32, activation='relu')(encoded)  # 병목층\n",
    "        \n",
    "        # 디코더 - 차원 복원\n",
    "        decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        decoded = layers.BatchNormalization()(decoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        \n",
    "        decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "        decoded = layers.BatchNormalization()(decoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        \n",
    "        decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "        \n",
    "        autoencoder = keras.Model(input_layer, decoded)\n",
    "        autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "\n",
    "    def train_and_detect(self, df, feature_cols):\n",
    "        \"\"\"\n",
    "        모델 학습 및 이상탐지 수행\n",
    "        \"\"\"\n",
    "        print(\"모델 학습 및 이상탐지 시작...\")\n",
    "        \n",
    "        # 정상 데이터만 사용하여 학습\n",
    "        normal_data = df[~df['is_anomaly']]\n",
    "        X_normal = normal_data[feature_cols].values\n",
    "        \n",
    "        print(f\"학습 데이터: {len(X_normal)}개 정상 샘플\")\n",
    "        \n",
    "        # Autoencoder 학습\n",
    "        self.models['autoencoder'] = self.build_autoencoder(len(feature_cols))\n",
    "        \n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = self.models['autoencoder'].fit(\n",
    "            X_normal, X_normal,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            epochs=self.config['epochs'],\n",
    "            validation_split=self.config['validation_split'],\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.history['autoencoder'] = history\n",
    "        \n",
    "        # 임계값 계산\n",
    "        reconstructed = self.models['autoencoder'].predict(X_normal, verbose=0)\n",
    "        mse_normal = np.mean(np.square(X_normal - reconstructed), axis=1)\n",
    "        self.threshold = np.percentile(mse_normal, self.config['threshold_percentile'])\n",
    "        \n",
    "        print(f\"이상탐지 임계값: {self.threshold:.4f}\")\n",
    "        \n",
    "        # 전체 데이터에 대해 이상탐지\n",
    "        X_all = df[feature_cols].values\n",
    "        reconstructed_all = self.models['autoencoder'].predict(X_all, verbose=0)\n",
    "        mse_scores = np.mean(np.square(X_all - reconstructed_all), axis=1)\n",
    "        \n",
    "        # 결과 저장\n",
    "        df_result = df.copy()\n",
    "        df_result['anomaly_score'] = mse_scores\n",
    "        df_result['predicted_anomaly'] = mse_scores > self.threshold\n",
    "        df_result['confidence'] = (mse_scores - self.threshold) / self.threshold\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "    def analyze_results(self, df_result):\n",
    "        \"\"\"\n",
    "        결과 분석 및 요약\n",
    "        \"\"\"\n",
    "        print(\"결과 분석 중...\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        total_lots = len(df_result)\n",
    "        detected_anomalies = df_result['predicted_anomaly'].sum()\n",
    "        actual_anomalies = df_result['is_anomaly'].sum()\n",
    "        \n",
    "        print(f\"\\n=== 이상탐지 결과 요약 ===\")\n",
    "        print(f\"전체 LOT 수: {total_lots}\")\n",
    "        print(f\"실제 이상 LOT: {actual_anomalies} ({actual_anomalies/total_lots:.1%})\")\n",
    "        print(f\"탐지된 이상 LOT: {detected_anomalies} ({detected_anomalies/total_lots:.1%})\")\n",
    "        \n",
    "        # 성능 평가 (실제 라벨이 있는 경우)\n",
    "        if 'is_anomaly' in df_result.columns:\n",
    "            from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "            \n",
    "            precision = precision_score(df_result['is_anomaly'], df_result['predicted_anomaly'])\n",
    "            recall = recall_score(df_result['is_anomaly'], df_result['predicted_anomaly'])\n",
    "            f1 = f1_score(df_result['is_anomaly'], df_result['predicted_anomaly'])\n",
    "            \n",
    "            print(f\"\\n=== 성능 지표 ===\")\n",
    "            print(f\"정밀도 (Precision): {precision:.3f}\")\n",
    "            print(f\"재현율 (Recall): {recall:.3f}\")\n",
    "            print(f\"F1 점수: {f1:.3f}\")\n",
    "        \n",
    "        # 이상 LOT 상세 분석\n",
    "        anomaly_lots = df_result[df_result['predicted_anomaly']]\n",
    "        if len(anomaly_lots) > 0:\n",
    "            print(f\"\\n=== 이상 LOT 분석 ===\")\n",
    "            print(\"이상 점수가 높은 상위 5개 LOT:\")\n",
    "            top_anomalies = anomaly_lots.nlargest(5, 'anomaly_score')\n",
    "            for _, row in top_anomalies.iterrows():\n",
    "                print(f\"  LOT {row['LOT_NO']}: 점수 {row['anomaly_score']:.4f}\")\n",
    "                if 'FINAL_YIELD' in row:\n",
    "                    print(f\"    수율: {row['FINAL_YIELD']:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'total_lots': total_lots,\n",
    "            'detected_anomalies': detected_anomalies,\n",
    "            'detection_rate': detected_anomalies / total_lots,\n",
    "            'anomaly_lots': anomaly_lots\n",
    "        }\n",
    "\n",
    "    def visualize_results(self, df_result):\n",
    "        \"\"\"\n",
    "        결과 시각화\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. 이상 점수 분포\n",
    "        normal_scores = df_result[~df_result['predicted_anomaly']]['anomaly_score']\n",
    "        anomaly_scores = df_result[df_result['predicted_anomaly']]['anomaly_score']\n",
    "        \n",
    "        axes[0, 0].hist(normal_scores, bins=50, alpha=0.7, label='Normal', color='blue')\n",
    "        axes[0, 0].hist(anomaly_scores, bins=50, alpha=0.7, label='Anomaly', color='red')\n",
    "        axes[0, 0].axvline(self.threshold, color='red', linestyle='--', label='Threshold')\n",
    "        axes[0, 0].set_xlabel('Anomaly Score')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Anomaly Score Distribution')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. 수율 vs 이상 점수 (수율 정보가 있는 경우)\n",
    "        if 'FINAL_YIELD' in df_result.columns:\n",
    "            scatter = axes[0, 1].scatter(\n",
    "                df_result['FINAL_YIELD'], \n",
    "                df_result['anomaly_score'],\n",
    "                c=df_result['predicted_anomaly'], \n",
    "                cmap='coolwarm', \n",
    "                alpha=0.6\n",
    "            )\n",
    "            axes[0, 1].set_xlabel('Final Yield (%)')\n",
    "            axes[0, 1].set_ylabel('Anomaly Score')\n",
    "            axes[0, 1].set_title('Yield vs Anomaly Score')\n",
    "            plt.colorbar(scatter, ax=axes[0, 1])\n",
    "        \n",
    "        # 3. 혼동 행렬\n",
    "        if 'is_anomaly' in df_result.columns:\n",
    "            cm = confusion_matrix(df_result['is_anomaly'], df_result['predicted_anomaly'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 0], cmap='Blues')\n",
    "            axes[1, 0].set_title('Confusion Matrix')\n",
    "            axes[1, 0].set_xlabel('Predicted')\n",
    "            axes[1, 0].set_ylabel('Actual')\n",
    "        \n",
    "        # 4. 학습 손실\n",
    "        if 'autoencoder' in self.history:\n",
    "            history = self.history['autoencoder']\n",
    "            axes[1, 1].plot(history.history['loss'], label='Training Loss')\n",
    "            axes[1, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Loss')\n",
    "            axes[1, 1].set_title('Training Loss')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def detect_anomalies_in_timerange(self, df_result, start_time=None, end_time=None):\n",
    "        \"\"\"\n",
    "        특정 시간 구간의 이상탐지 결과 조회\n",
    "        \"\"\"\n",
    "        if start_time is None and end_time is None:\n",
    "            print(\"전체 기간 이상탐지 결과:\")\n",
    "            filtered_df = df_result\n",
    "        else:\n",
    "            print(f\"시간 구간 ({start_time} ~ {end_time}) 이상탐지 결과:\")\n",
    "            # 실제 시간 컬럼이 있다면 필터링 적용\n",
    "            # 여기서는 LOT_NO 기준으로 간단히 필터링\n",
    "            filtered_df = df_result\n",
    "        \n",
    "        anomalies = filtered_df[filtered_df['predicted_anomaly']]\n",
    "        \n",
    "        print(f\"조회 기간 내 이상 LOT: {len(anomalies)}개\")\n",
    "        \n",
    "        if len(anomalies) > 0:\n",
    "            print(\"\\n이상 LOT 목록:\")\n",
    "            for _, row in anomalies.iterrows():\n",
    "                print(f\"  - LOT {row['LOT_NO']}: 이상점수 {row['anomaly_score']:.4f}\")\n",
    "                if 'FINAL_YIELD' in row:\n",
    "                    print(f\"    수율: {row['FINAL_YIELD']:.1f}%\")\n",
    "        \n",
    "        return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f96502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 반도체 데이터 이상탐지 시스템 시작!\n",
      "============================================================\n",
      "실제 반도체 데이터 로딩 및 탐색 중...\n",
      "로딩 중: SEMI_LOT_MANAGE.csv\n",
      "  - 크기: (40, 12)\n",
      "  - 컬럼: ['PNO', 'LOT_NO', 'PRODUCT_NAME', 'RECIPE_ID', 'START_QTY', 'CURRENT_STEP', 'PRIORITY', 'CREDATE', 'HOLDER', 'FINAL_YIELD', 'GOOD_DIE', 'TOTAL_DIE']\n",
      "\n",
      "로딩 중: SEMI_PROCESS_HISTORY.csv\n",
      "  - 크기: (50, 10)\n",
      "  - 컬럼: ['PNO', 'LOT_NO', 'PROCESS_STEP', 'EQUIPMENT_ID', 'RECIPE_ID', 'OPERATOR', 'START_TIME', 'END_TIME', 'IN_QTY', 'OUT_QTY']\n",
      "\n",
      "로딩 중: SEMI_PARAM_MEASURE.csv\n",
      "  - 크기: (50, 13)\n",
      "  - 컬럼: ['PNO', 'LOT_NO', 'WAFER_ID', 'PROCESS_STEP', 'EQUIPMENT_ID', 'CATEGORY', 'PARAM_NAME', 'UNIT', 'MEASURED_VAL', 'TARGET_VAL', 'USL', 'LSL', 'MEASURE_TIME']\n",
      "\n",
      "로딩 중: SEMI_EQUIPMENT_SENSOR.csv\n",
      "  - 크기: (70, 8)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'SENSOR_TYPE', 'SENSOR_VALUE', 'TIMESTAMP', 'CHAMBER_ID', 'RECIPE_STEP']\n",
      "\n",
      "로딩 중: SEMI_SENSOR_ALERT_CONFIG.csv\n",
      "  - 크기: (50, 12)\n",
      "  - 컬럼: ['CONFIG_ID', 'SENSOR_ID', 'PARAM_NAME', 'WARNING_UPPER', 'WARNING_LOWER', 'ALARM_UPPER', 'ALARM_LOWER', 'INTERLOCK_UPPER', 'INTERLOCK_LOWER', 'MOVING_AVG_WINDOW', 'ALERT_TYPE', 'ENABLED']\n",
      "\n",
      "로딩 중: SEMI_PHOTO_SENSORS.csv\n",
      "  - 크기: (35, 15)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'WAFER_ID', 'TIMESTAMP', 'EXPOSURE_DOSE', 'FOCUS_POSITION', 'STAGE_TEMP', 'BAROMETRIC_PRESSURE', 'HUMIDITY', 'ALIGNMENT_ERROR_X', 'ALIGNMENT_ERROR_Y', 'LENS_ABERRATION', 'ILLUMINATION_UNIFORMITY', 'RETICLE_TEMP']\n",
      "\n",
      "로딩 중: SEMI_ETCH_SENSORS.csv\n",
      "  - 크기: (34, 16)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'TIMESTAMP', 'RF_POWER_SOURCE', 'RF_POWER_BIAS', 'CHAMBER_PRESSURE', 'GAS_FLOW_CF4', 'GAS_FLOW_O2', 'GAS_FLOW_AR', 'GAS_FLOW_CL2', 'ELECTRODE_TEMP', 'CHAMBER_WALL_TEMP', 'HELIUM_PRESSURE', 'ENDPOINT_SIGNAL', 'PLASMA_DENSITY']\n",
      "\n",
      "로딩 중: SEMI_CVD_SENSORS.csv\n",
      "  - 크기: (33, 15)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'TIMESTAMP', 'SUSCEPTOR_TEMP', 'CHAMBER_PRESSURE', 'PRECURSOR_FLOW_TEOS', 'PRECURSOR_FLOW_SILANE', 'PRECURSOR_FLOW_WF6', 'CARRIER_GAS_N2', 'CARRIER_GAS_H2', 'SHOWERHEAD_TEMP', 'LINER_TEMP', 'DEPOSITION_RATE', 'FILM_STRESS']\n",
      "\n",
      "로딩 중: SEMI_IMPLANT_SENSORS.csv\n",
      "  - 크기: (27, 15)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'TIMESTAMP', 'BEAM_CURRENT', 'BEAM_ENERGY', 'DOSE_RATE', 'TOTAL_DOSE', 'IMPLANT_ANGLE', 'WAFER_ROTATION', 'SOURCE_PRESSURE', 'ANALYZER_PRESSURE', 'END_STATION_PRESSURE', 'BEAM_UNIFORMITY', 'FARADAY_CUP_CURRENT']\n",
      "\n",
      "로딩 중: SEMI_CMP_SENSORS.csv\n",
      "  - 크기: (26, 15)\n",
      "  - 컬럼: ['PNO', 'EQUIPMENT_ID', 'LOT_NO', 'TIMESTAMP', 'HEAD_PRESSURE', 'RETAINER_PRESSURE', 'PLATEN_ROTATION', 'HEAD_ROTATION', 'SLURRY_FLOW_RATE', 'SLURRY_TEMP', 'PAD_TEMP', 'REMOVAL_RATE', 'MOTOR_CURRENT', 'CONDITIONER_PRESSURE', 'ENDPOINT_SIGNAL']\n",
      "\n",
      "통합 데이터셋 생성 중...\n",
      "센서 데이터 통합 중...\n",
      "  - photo_sensors: 10개 센서, 35개 레코드\n",
      "  - etch_sensors: 12개 센서, 34개 레코드\n",
      "  - cvd_sensors: 11개 센서, 33개 레코드\n",
      "  - implant_sensors: 11개 센서, 27개 레코드\n",
      "  - cmp_sensors: 11개 센서, 26개 레코드\n",
      "통합 완료: 총 1704개 센서 레코드\n",
      "기본 LOT 데이터: 40개 LOT\n",
      "센서 특성 추가 완료: 212개 특성\n",
      "공정 이력 특성 추가 완료\n",
      "파라미터 측정 특성 추가 완료\n",
      "최종 통합 데이터셋: (40, 232)\n",
      "특성 준비 및 전처리 중...\n",
      "전처리 완료: 223개 특성\n",
      "이상 LOT 비율: 10.00%\n",
      "모델 학습 및 이상탐지 시작...\n",
      "학습 데이터: 36개 정상 샘플\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m processed_df, feature_cols = detector.prepare_features(unified_df)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 5. 모델 학습 및 이상탐지\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m result_df = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 6. 결과 분석\u001b[39;00m\n\u001b[32m     26\u001b[39m analysis = detector.analyze_results(result_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 297\u001b[39m, in \u001b[36mSemiconductorRealDataDetector.train_and_detect\u001b[39m\u001b[34m(self, df, feature_cols)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m학습 데이터: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_normal)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m개 정상 샘플\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Autoencoder 학습\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.models[\u001b[33m'\u001b[39m\u001b[33mautoencoder\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m early_stopping = keras.callbacks.EarlyStopping(\n\u001b[32m    300\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    301\u001b[39m )\n\u001b[32m    303\u001b[39m history = \u001b[38;5;28mself\u001b[39m.models[\u001b[33m'\u001b[39m\u001b[33mautoencoder\u001b[39m\u001b[33m'\u001b[39m].fit(\n\u001b[32m    304\u001b[39m     X_normal, X_normal,\n\u001b[32m    305\u001b[39m     batch_size=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m    310\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 254\u001b[39m, in \u001b[36mSemiconductorRealDataDetector.build_autoencoder\u001b[39m\u001b[34m(self, input_dim)\u001b[39m\n\u001b[32m    251\u001b[39m input_layer = layers.Input(shape=(input_dim,))\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# 인코더 - 차원 축소\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m encoded = \u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m encoded = layers.BatchNormalization()(encoded)\n\u001b[32m    256\u001b[39m encoded = layers.Dropout(\u001b[32m0.2\u001b[39m)(encoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/agi/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/agi/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:151\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf.is_tensor(x):\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype == \u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_int_dtype(dtype):\n\u001b[32m    148\u001b[39m         \u001b[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001b[39;00m\n\u001b[32m    149\u001b[39m         \u001b[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001b[39;00m\n\u001b[32m    150\u001b[39m         \u001b[38;5;66;03m# and cast instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m         x = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.cast(x, dtype)\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.convert_to_tensor(x, dtype=dtype)\n",
      "\u001b[31mRuntimeError\u001b[39m: Bad StatusOr access: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"실제 반도체 데이터 이상탐지 시스템 시작!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 데이터 경로 설정\n",
    "    data_path = \"/home/minjoo/PRISM-Monitor/prism_monitor/data/Industrial_DB_sample\"\n",
    "    \n",
    "    # 1. 탐지기 초기화\n",
    "    detector = SemiconductorRealDataDetector(data_path)\n",
    "    \n",
    "    # 2. 데이터 탐색\n",
    "    datasets = detector.load_and_explore_data()\n",
    "    \n",
    "    # 3. 데이터 통합\n",
    "    unified_df = detector.create_unified_dataset(datasets)\n",
    "    \n",
    "    if not unified_df.empty:\n",
    "        # 4. 특성 준비\n",
    "        processed_df, feature_cols = detector.prepare_features(unified_df)\n",
    "        \n",
    "        # 5. 모델 학습 및 이상탐지\n",
    "        result_df = detector.train_and_detect(processed_df, feature_cols)\n",
    "        \n",
    "        # 6. 결과 분석\n",
    "        analysis = detector.analyze_results(result_df)\n",
    "        \n",
    "        # 7. 시각화\n",
    "        detector.visualize_results(result_df)\n",
    "        \n",
    "        # 8. 특정 구간 이상탐지 조회 예시\n",
    "        anomalies = detector.detect_anomalies_in_timerange(result_df)\n",
    "        \n",
    "        print(\"\\n이상탐지 완료!\")\n",
    "    else:\n",
    "        print(\"데이터 통합에 실패했습니다. 데이터 구조를 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b40a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
