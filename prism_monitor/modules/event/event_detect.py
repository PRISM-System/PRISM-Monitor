import os
from glob import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.ensemble import IsolationForest
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import warnings
import os
import json
import io
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from tinydb import TinyDB, Query
import matplotlib
matplotlib.use('Agg')

warnings.filterwarnings('ignore')

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • (CuDNN ë²„ì „ ë¶ˆì¼ì¹˜ ìš°íšŒ)
try:
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ“ GPU ë©”ëª¨ë¦¬ ì¦ê°€ ëª¨ë“œ í™œì„±í™” ({len(gpus)}ê°œ GPU)")
except Exception as e:
    print(f"GPU ì„¤ì • ì¤‘ ê²½ê³ : {e}")
    # GPU ì„¤ì • ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰

class ModelManager:
    """
    ëª¨ë¸ ì €ì¥, ë¡œë”©, ê´€ë¦¬ í´ë˜ìŠ¤ (ê°œì„ ëœ ë²„ì „)
    """
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        self.model_metadata_file = os.path.join(model_dir, "model_metadata.json")
        self.scaler_file = os.path.join(model_dir, "scaler.pkl")
        
    def save_model(self, model, scaler, feature_cols: List[str], threshold: float, 
                   training_data_info: Dict, performance_metrics: Dict):
        """
        ëª¨ë¸ê³¼ ê´€ë ¨ ì •ë³´ ì €ì¥ (ë‹¤ì¤‘ ì„ê³„ê°’ ì§€ì›ìœ¼ë¡œ ê°œì„ )
        """
        try:
            model_file = os.path.join(self.model_dir, "autoencoder_model.h5")
            model.save(model_file)
            
            with open(self.scaler_file, 'wb') as f:
                pickle.dump(scaler, f)
            
            # ì„ê³„ê°’ì´ dictì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(threshold, dict):
                thresholds = threshold
            else:
                thresholds = {'default': threshold, 'legacy': threshold}
            
            metadata = {
                'model_version': datetime.now().strftime('%Y%m%d_%H%M%S'),
                'training_timestamp': datetime.now().isoformat(),
                'feature_columns': feature_cols,
                'threshold': threshold,  # ê¸°ì¡´ í˜¸í™˜ì„±
                'thresholds': thresholds,  # ìƒˆë¡œìš´ ë‹¤ì¤‘ ì„ê³„ê°’
                'training_data_info': training_data_info,
                'performance_metrics': performance_metrics,
                'model_file': model_file,
                'scaler_file': self.scaler_file
            }
            
            with open(self.model_metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
            
            print(f"ê°œì„ ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file}")
            if isinstance(threshold, dict):
                print(f"ë‹¤ì¤‘ ì„ê³„ê°’: {threshold}")
            else:
                print(f"ì„ê³„ê°’: {threshold}")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self) -> Tuple[Optional[keras.Model], Optional[StandardScaler], Optional[Dict]]:
        """
        ì €ì¥ëœ ëª¨ë¸ê³¼ ê´€ë ¨ ì •ë³´ ë¡œë“œ
        """
        try:
            if not os.path.exists(self.model_metadata_file):
                print("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
            
            with open(self.model_metadata_file, 'r') as f:
                metadata = json.load(f)
            
            model_file = metadata.get('model_file')
            if not os.path.exists(model_file):
                print(f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_file}")
                return None, None, None
            
            from tensorflow.keras.metrics import MeanSquaredError
            model = keras.models.load_model(model_file, custom_objects={"mse": MeanSquaredError()})
            
            scaler = None
            if os.path.exists(self.scaler_file):
                with open(self.scaler_file, 'rb') as f:
                    scaler = pickle.load(f)
            
            print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {metadata['model_version']}")
            return model, scaler, metadata
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def is_model_available(self) -> bool:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        """
        model, scaler, metadata = self.load_model()
        return model is not None and scaler is not None

class NormalStateManager:
    """
    ì •ìƒ ìƒíƒœ ë°ì´í„° ê´€ë¦¬ ëª¨ë“ˆ (drift ì‹œê°í™” ê°œì„ )
    """
    
    def __init__(self, storage_path: str = "normal_state_profiles.json"):
        self.storage_path = storage_path
        self.normal_profiles = self.load_profiles()
    
    def load_profiles(self) -> Dict:
        """ì €ì¥ëœ ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ë¡œë“œ"""
        try:
            if os.path.exists(self.storage_path):
                with open(self.storage_path, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def save_profiles(self):
        """ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ì €ì¥"""
        try:
            with open(self.storage_path, 'w') as f:
                json.dump(self.normal_profiles, f, indent=2, default=str)
        except Exception as e:
            print(f"ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def update_normal_profile(self, equipment_id: str, process_step: str, data: pd.DataFrame):
        """
        ì¥ë¹„ë³„/ê³µì •ë³„ ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸
        """
        profile_key = f"{equipment_id}_{process_step}"
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        
        profile = {
            'equipment_id': equipment_id,
            'process_step': process_step,
            'last_updated': datetime.now().isoformat(),
            'sample_count': len(data),
            'statistics': {}
        }
        
        for col in numeric_cols:
            if col in data.columns and not data[col].empty:
                profile['statistics'][col] = {
                    'mean': float(data[col].mean()),
                    'std': float(data[col].std()),
                    'median': float(data[col].median()),
                    'min': float(data[col].min()),
                    'max': float(data[col].max()),
                    'q25': float(data[col].quantile(0.25)),
                    'q75': float(data[col].quantile(0.75))
                }
        
        self.normal_profiles[profile_key] = profile
        self.save_profiles()
        
        print(f"ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ì—…ë°ì´íŠ¸: {profile_key} (ìƒ˜í”Œ {len(data)}ê°œ)")
        return profile
    
    def get_normal_profile(self, equipment_id: str, process_step: str) -> Optional[Dict]:
        """ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ì¡°íšŒ"""
        profile_key = f"{equipment_id}_{process_step}"
        return self.normal_profiles.get(profile_key)
    
    def detect_profile_drift(self, equipment_id: str, process_step: str, current_data: pd.DataFrame) -> Dict:
        """
        ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ë³€í™” ê°ì§€
        """
        profile = self.get_normal_profile(equipment_id, process_step)
        if not profile:
            return {'status': 'no_profile', 'message': 'ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}
        
        drift_results = {
            'equipment_id': equipment_id,
            'process_step': process_step,
            'drift_detected': False,
            'drift_parameters': [],
            'drift_score': 0,
            'check_timestamp': datetime.now().isoformat()
        }
        
        numeric_cols = current_data.select_dtypes(include=[np.number]).columns
        drift_count = 0
        total_params = 0
        
        for col in numeric_cols:
            if col in profile['statistics'] and not current_data[col].empty:
                total_params += 1
                current_mean = current_data[col].mean()
                normal_mean = profile['statistics'][col]['mean']
                normal_std = profile['statistics'][col]['std']
                
                # Z-score ê³„ì‚° (3-sigma ë£°)
                if normal_std > 0:
                    z_score = abs((current_mean - normal_mean) / normal_std)
                    if z_score > 3:  # 3-sigmaë¥¼ ë²—ì–´ë‚˜ë©´ drift ê°ì§€
                        drift_count += 1
                        drift_results['drift_parameters'].append({
                            'parameter': col,
                            'current_value': float(current_mean),
                            'normal_value': float(normal_mean),
                            'z_score': float(z_score),
                            'severity': 'HIGH' if z_score > 5 else 'MEDIUM'
                        })
        
        if total_params > 0:
            drift_results['drift_score'] = (drift_count / total_params) * 100
            drift_results['drift_detected'] = drift_results['drift_score'] > 10  # 10% ì´ìƒ íŒŒë¼ë¯¸í„°ì— drift
        
        return drift_results
    
    def get_all_profiles_summary(self) -> Dict:
        """ëª¨ë“  ì •ìƒ ìƒíƒœ í”„ë¡œíŒŒì¼ ìš”ì•½"""
        return {
            'total_profiles': len(self.normal_profiles),
            'profiles': [
                {
                    'key': key,
                    'equipment_id': profile['equipment_id'],
                    'process_step': profile['process_step'],
                    'last_updated': profile['last_updated'],
                    'sample_count': profile['sample_count']
                }
                for key, profile in self.normal_profiles.items()
            ]
        }
        
    def visualize_drift_results(self, drift_results: List[Dict]) -> str:
        """
        í”„ë¡œíŒŒì¼ ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ì‹œê°í™” (SVG) - ê°œì„ ëœ ë²„ì „
        """
        print(f"ë“œë¦¬í”„íŠ¸ ì‹œê°í™” ì‹œì‘: {len(drift_results)}ê°œ ë“œë¦¬í”„íŠ¸ ê²°ê³¼")
        
        if not drift_results:
            # ë¹ˆ ë“œë¦¬í”„íŠ¸ ê²°ê³¼ì— ëŒ€í•œ ê¸°ë³¸ SVG
            svg_content = '''
            <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
                <rect width="800" height="400" fill="white" stroke="black" stroke-width="1"/>
                <text x="400" y="200" text-anchor="middle" font-size="18" fill="green">
                    ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
                </text>
                <text x="400" y="230" text-anchor="middle" font-size="14" fill="gray">
                    ëª¨ë“  ì¥ë¹„ê°€ ì •ìƒ ìƒíƒœë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
                </text>
            </svg>
            '''
            return svg_content.strip()
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. ì¥ë¹„ë³„ ë“œë¦¬í”„íŠ¸ ì ìˆ˜
            equipment_scores = {}
            for drift in drift_results:
                equipment_id = drift.get('equipment_id', 'Unknown')
                if equipment_id not in equipment_scores:
                    equipment_scores[equipment_id] = []
                equipment_scores[equipment_id].append(drift.get('drift_score', 0))
            
            if equipment_scores:
                equipment_names = list(equipment_scores.keys())
                avg_scores = [np.mean(scores) for scores in equipment_scores.values()]
                
                colors = ['red' if score > 50 else 'orange' if score > 20 else 'yellow' for score in avg_scores]
                axes[0, 0].bar(equipment_names, avg_scores, color=colors, alpha=0.7)
                axes[0, 0].set_title('Equipment Drift Scores', fontsize=12, fontweight='bold')
                axes[0, 0].set_ylabel('Average Drift Score (%)')
                axes[0, 0].tick_params(axis='x', rotation=45)
                axes[0, 0].grid(True, alpha=0.3)
                
                # ì„ê³„ê°’ ë¼ì¸ ì¶”ê°€
                axes[0, 0].axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='Warning (10%)')
                axes[0, 0].axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Critical (50%)')
                axes[0, 0].legend()
            else:
                axes[0, 0].text(0.5, 0.5, 'No equipment data', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Equipment Drift Scores')
            
            # 2. ì‹¬ê°ë„ë³„ ë¶„í¬
            severity_counts = {'HIGH': 0, 'MEDIUM': 0}
            for drift in drift_results:
                for param in drift.get('drift_parameters', []):
                    severity = param.get('severity', 'MEDIUM')
                    if severity in severity_counts:
                        severity_counts[severity] += 1
                    else:
                        severity_counts['MEDIUM'] += 1
            
            total_severity = sum(severity_counts.values())
            if total_severity > 0:
                colors_pie = ['red', 'orange']
                wedges, texts, autotexts = axes[0, 1].pie(
                    severity_counts.values(), 
                    labels=severity_counts.keys(), 
                    autopct='%1.1f%%', 
                    colors=colors_pie,
                    startangle=90
                )
                axes[0, 1].set_title('Drift Severity Distribution', fontsize=12, fontweight='bold')
                
                # í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ê°œì„ 
                for autotext in autotexts:
                    autotext.set_color('white')
                    autotext.set_fontweight('bold')
            else:
                axes[0, 1].text(0.5, 0.5, 'No severity data', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Drift Severity Distribution')
            
            # 3. ì‹œê°„ë³„ ë“œë¦¬í”„íŠ¸ ë°œìƒ ì¶”ì´
            try:
                drift_times = []
                for drift in drift_results:
                    timestamp = drift.get('check_timestamp')
                    if timestamp:
                        try:
                            drift_times.append(pd.to_datetime(timestamp))
                        except:
                            drift_times.append(datetime.now())
                
                if drift_times:
                    time_df = pd.DataFrame({'timestamp': drift_times})
                    time_df['hour'] = time_df['timestamp'].dt.floor('H')
                    time_counts = time_df.groupby('hour').size()
                    
                    if len(time_counts) > 0:
                        axes[1, 0].plot(time_counts.index, time_counts.values, 
                                       marker='o', color='red', linewidth=2, markersize=6)
                        axes[1, 0].fill_between(time_counts.index, time_counts.values, 
                                              alpha=0.3, color='red')
                        axes[1, 0].set_title('Drift Detection Over Time', fontsize=12, fontweight='bold')
                        axes[1, 0].set_xlabel('Time')
                        axes[1, 0].set_ylabel('Number of Drifts')
                        axes[1, 0].tick_params(axis='x', rotation=45)
                        axes[1, 0].grid(True, alpha=0.3)
                    else:
                        axes[1, 0].text(0.5, 0.5, 'No time data', ha='center', va='center', 
                                       transform=axes[1, 0].transAxes)
                else:
                    axes[1, 0].text(0.5, 0.5, 'No time data', ha='center', va='center', 
                                   transform=axes[1, 0].transAxes)
                    axes[1, 0].set_title('Drift Detection Over Time')
            except Exception as e:
                print(f"ì‹œê°„ë³„ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
                axes[1, 0].text(0.5, 0.5, 'Time chart error', ha='center', va='center', 
                               transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Drift Detection Over Time')
            
            # 4. íŒŒë¼ë¯¸í„°ë³„ Z-score ë¶„í¬ (ê°œì„ ë¨)
            z_scores = []
            param_names = []
            colors_scatter = []
            
            for drift in drift_results:
                for param in drift.get('drift_parameters', []):
                    z_score = param.get('z_score', 0)
                    param_name = param.get('parameter', 'Unknown')[:15]
                    severity = param.get('severity', 'MEDIUM')
                    
                    z_scores.append(z_score)
                    param_names.append(param_name)
                    colors_scatter.append('red' if severity == 'HIGH' else 'orange')
            
            if z_scores:
                scatter = axes[1, 1].scatter(range(len(z_scores)), z_scores, 
                                           c=colors_scatter, alpha=0.7, s=60, edgecolors='black')
                
                # ì„ê³„ê°’ ë¼ì¸ë“¤
                axes[1, 1].axhline(y=3, color='orange', linestyle='--', alpha=0.7, 
                                  linewidth=2, label='3-sigma threshold')
                axes[1, 1].axhline(y=5, color='red', linestyle='--', alpha=0.7, 
                                  linewidth=2, label='5-sigma threshold')
                
                axes[1, 1].set_title('Parameter Z-scores', fontsize=12, fontweight='bold')
                axes[1, 1].set_xlabel('Parameter Index')
                axes[1, 1].set_ylabel('Z-score')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
                
                # Yì¶• ë²”ìœ„ ì¡°ì •
                if max(z_scores) > 0:
                    axes[1, 1].set_ylim(0, max(z_scores) * 1.1)
            else:
                axes[1, 1].text(0.5, 0.5, 'No Z-score data', ha='center', va='center', 
                               transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Parameter Z-scores')
            
            plt.tight_layout()
            plt.subplots_adjust(hspace=0.3, wspace=0.3)
            
            # SVG ìƒì„±
            svg_buffer = io.StringIO()
            plt.savefig(svg_buffer, format='svg', bbox_inches='tight', 
                       facecolor='white', edgecolor='none', dpi=100)
            svg_content = svg_buffer.getvalue()
            svg_buffer.close()
            plt.close(fig)
            
            print(f"ë“œë¦¬í”„íŠ¸ SVG ì‹œê°í™” ìƒì„± ì™„ë£Œ: {len(svg_content)} ë¬¸ì")
            return svg_content
            
        except Exception as e:
            print(f"ë“œë¦¬í”„íŠ¸ ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ SVG ë°˜í™˜
            error_svg = f'''
            <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
                <rect width="800" height="400" fill="white" stroke="black" stroke-width="1"/>
                <text x="400" y="180" text-anchor="middle" font-size="16" fill="red">
                    ë“œë¦¬í”„íŠ¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
                </text>
                <text x="400" y="210" text-anchor="middle" font-size="12" fill="gray">
                    ì˜¤ë¥˜: {str(e)[:50]}...
                </text>
                <text x="400" y="240" text-anchor="middle" font-size="12" fill="blue">
                    ê°ì§€ëœ ë“œë¦¬í”„íŠ¸: {len(drift_results)}ê°œ
                </text>
            </svg>
            '''
            return error_svg.strip()

class DataValidityChecker:
    """
    ì‹¤ì‹œê°„ ë°ì´í„° ì •í•©ì„± ê²€ì¦ ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)
    """
    
    def __init__(self):
        # ê° ê³µì •ë³„ ì •ìƒ ë²”ìœ„ ì •ì˜ (ì‹ ê·œ 4ê°œ ë°˜ë„ì²´ ê³µì • ê¸°ì¤€)
        self.normal_ranges = {
            'semiconductor_full_004': {
                'rf_power': (800, 1200),
                'pressure': (60, 100),
                'temperature': (20, 60),
                'gas_flow_rate': (80, 220),
                'vacuum_pump': (70, 110),
                'plasma_density': (1e9, 5e10),
                'electron_temp': (1, 6),
                'process_yield': (90, 100),
                'defect_count': (0, 10),
                'compliance_status': (0, 1)
            },
            'semiconductor_etch_002': {
                'pressure': (60, 100),
                'vacuum_pump': (70, 110),
                'gas_flow_rate': (80, 220),
                'rf_power': (850, 1150),
                'temperature': (15, 60),
                'etch_rate': (100, 300),
                'bias_voltage': (-220, -60),
                'chamber_humidity': (0, 1),
                'gas_composition': (0, 1)
            },
            'semiconductor_deposition_003': {
                'temperature': (300, 450),
                'pressure': (150, 320),
                'gas_flow_rate': (80, 260),
                'rf_power': (450, 750),
                'deposition_rate': (60, 140),
                'film_thickness': (20, 90),
                'substrate_temp': (320, 460),
                'precursor_flow': (10, 70),
                'uniformity': (70, 100)
            },
            'semiconductor_cmp_001': {
                'motor_current': (5, 40),
                'slurry_flow_rate': (80, 350),
                'head_rotation': (20, 200),
                'pressure': (1, 6),
                'temperature': (15, 40),
                'polish_time': (100, 400),
                'pad_thickness': (0.5, 5),
                'slurry_temp': (10, 40),
                'vibration': (0, 5)
            }
        }
    
    def validate_data_integrity(self, df: pd.DataFrame, table_name: str) -> Dict:
        """
        ë°ì´í„° ì •í•©ì„± ê²€ì¦
        """
        validation_results = {
            'table_name': table_name,
            'total_records': len(df),
            'missing_values': {},
            'out_of_range_values': {},
            'data_quality_score': 0,
            'anomalies': [],
            'validation_timestamp': datetime.now().isoformat()
        }
        
        # 1. ê²°ì¸¡ì¹˜ ê²€ì‚¬
        missing_counts = df.isnull().sum()
        validation_results['missing_values'] = {
            col: int(count) for col, count in missing_counts.items() if count > 0
        }
        
        # 2. ë²”ìœ„ ê²€ì‚¬ (ì •ìƒ ë²”ìœ„ê°€ ì •ì˜ëœ ê²½ìš°)
        if table_name in self.normal_ranges:
            ranges = self.normal_ranges[table_name]
            for col, (min_val, max_val) in ranges.items():
                if col in df.columns:
                    out_of_range = df[(df[col] < min_val) | (df[col] > max_val)]
                    if len(out_of_range) > 0:
                        validation_results['out_of_range_values'][col] = {
                            'count': len(out_of_range),
                            'percentage': len(out_of_range) / len(df) * 100,
                            'expected_range': f"{min_val} - {max_val}",
                            'actual_range': f"{df[col].min():.3f} - {df[col].max():.3f}"
                        }
        
        # 3. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        total_issues = sum(len(issues) for issues in validation_results['out_of_range_values'].values())
        total_missing = sum(validation_results['missing_values'].values())
        total_problems = total_issues + total_missing
        
        if len(df) > 0:
            validation_results['data_quality_score'] = max(0, 100 - (total_problems / len(df) * 100))
        
        # 4. ì‹¬ê°í•œ ì´ìƒ ê²€ì¶œ
        critical_threshold = 5  # 5% ì´ìƒ ë¬¸ì œê°€ ìˆìœ¼ë©´ critical
        for col, info in validation_results['out_of_range_values'].items():
            if info['percentage'] > critical_threshold:
                validation_results['anomalies'].append({
                    'type': 'critical_out_of_range',
                    'column': col,
                    'severity': 'HIGH',
                    'description': f"{col}ì—ì„œ {info['percentage']:.1f}%ì˜ ë°ì´í„°ê°€ ì •ìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨"
                })
        
        return validation_results
    
    def preprocess_and_clean(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """
        ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
        """
        df_clean = df.copy()
        
        # 1. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        
        # 2. ê°œì„ ëœ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in numeric_cols:
            if df_clean[col].isnull().any():
                # Median ì‚¬ìš© (í‰ê· ë³´ë‹¤ ì´ìƒì¹˜ì— ê°•ê±´)
                median_val = df_clean[col].median()
                df_clean[col] = df_clean[col].fillna(median_val)
        
        # 3. IQR ê¸°ë°˜ ì´ìƒì¹˜ ì²˜ë¦¬ (ê°œì„ ëœ ë°©ë²•)
        for col in numeric_cols:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            
            if IQR > 0:
                # 3Ã—IQR ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ë” ê°•ê±´í•œ ë°©ë²•)
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR
                df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)
        
        # 4. ì •ìƒ ë²”ìœ„ ê¸°ë°˜ í´ë¦¬í•‘
        if table_name in self.normal_ranges:
            ranges = self.normal_ranges[table_name]
            for col, (min_val, max_val) in ranges.items():
                if col in df_clean.columns:
                    df_clean[col] = df_clean[col].clip(lower=min_val, upper=max_val)
        
        return df_clean

class EnhancedSemiconductorRealTimeMonitor:
    """
    í–¥ìƒëœ ë°˜ë„ì²´ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, model_dir: str = "models"):
        self.model_manager = ModelManager(model_dir)
        self.normal_state_manager = NormalStateManager()
        self.data_validator = DataValidityChecker()
        self.model = None
        self.scaler = None
        self.metadata = None
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.load_model()
    
    def load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model, self.scaler, self.metadata = self.model_manager.load_model()
            if self.model is None:
                print("Warning: ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    from datetime import datetime
from typing import List, Dict, Tuple


class RealtimeAnomalyDetector:
    def __init__(self, data_validator, normal_state_manager):
        """
        data_validator: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦ ê°ì²´
        normal_state_manager: ì •ìƒ ìƒíƒœ ê´€ë¦¬ ë° ë“œë¦¬í”„íŠ¸ ê°ì§€ ê°ì²´
        """
        self.data_validator = data_validator
        self.normal_state_manager = normal_state_manager

    def _fetch_data_from_database(self, prism_core_db, start: str, end: str) -> Dict[str, 'pd.DataFrame']:
        """
        DBì—ì„œ start ~ end êµ¬ê°„ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        - ë°˜í™˜ê°’: {í…Œì´ë¸”ëª…: DataFrame}
        """
        # TODO: ì‹¤ì œ DB ì—°ë™ ë¡œì§ êµ¬í˜„
        raise NotImplementedError("DB ì—°ë™ ë¡œì§ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def fast_anomaly_detection_with_realtime_data(
        self, prism_core_db, start: str, end: str
    ) -> Tuple[List[Dict], List[Dict], Dict, Dict]:
        """
        ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ì´ìƒíƒì§€ ìˆ˜í–‰
        ë°˜í™˜ê°’: anomalies, drift_results, analysis, vis_json
        """
        print(f"ì‹¤ì‹œê°„ ì´ìƒíƒì§€ ì‹œì‘: {start} ~ {end}")
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            all_data = self._fetch_data_from_database(prism_core_db, start, end)
            
            if not all_data:
                vis_json = {
                    "anomalies": [],
                    "drift_results": [],
                    "raw_data": {}
                }
                return [], [], {}, vis_json
            
            # 2. ì´ìƒíƒì§€ ìˆ˜í–‰
            anomalies = []
            drift_results = []
            analysis_summary = {
                'total_records': 0,
                'tables_processed': 0,
                'anomalies_detected': 0,
                'drift_detected': 0,
                'processing_time': datetime.now().isoformat()
            }
            
            for table_name, data in all_data.items():
                print(f"ì²˜ë¦¬ ì¤‘ì¸ í…Œì´ë¸”: {table_name}, ë°ì´í„° ìˆ˜: {len(data)}")
                
                if data.empty:
                    continue
                
                analysis_summary['total_records'] += len(data)
                analysis_summary['tables_processed'] += 1
                
                # ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
                validated_data = self.data_validator.preprocess_and_clean(data, table_name)
                
                # ì´ìƒíƒì§€ ìˆ˜í–‰
                table_anomalies = self._detect_anomalies_in_data(validated_data, table_name)
                anomalies.extend(table_anomalies)
                
                # ë“œë¦¬í”„íŠ¸ ê°ì§€ (ì¥ë¹„ë³„ë¡œ ìˆ˜í–‰)
                if 'equipment_id' in data.columns:
                    for equipment_id in data['equipment_id'].unique():
                        equipment_data = data[data['equipment_id'] == equipment_id]
                        drift_result = self.normal_state_manager.detect_profile_drift(
                            equipment_id, table_name, equipment_data
                        )
                        if drift_result.get('drift_detected'):
                            drift_results.append(drift_result)
            
            analysis_summary['anomalies_detected'] = len(anomalies)
            analysis_summary['drift_detected'] = len(drift_results)
            
            # 3. raw ë°ì´í„° vis_json ìƒì„±
            vis_json = {
                "anomalies": anomalies,
                "drift_results": drift_results,
                "raw_data": {tbl: df.to_dict(orient="records") for tbl, df in all_data.items()}
            }
            
            print(f"ì´ìƒíƒì§€ ì™„ë£Œ: ì´ìƒ {len(anomalies)}ê°œ, ë“œë¦¬í”„íŠ¸ {len(drift_results)}ê°œ")
            
            return anomalies, drift_results, analysis_summary, vis_json
            
        except Exception as e:
            print(f"ì‹¤ì‹œê°„ ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            error_analysis = {
                'error': str(e),
                'processing_time': datetime.now().isoformat(),
                'status': 'error'
            }
            vis_json = {
                "anomalies": [],
                "drift_results": [],
                "raw_data": {},
                "error": str(e)
            }
            return [], [], error_analysis, vis_json

    def _fetch_data_from_database(self, prism_core_db, start: str, end: str) -> Dict[str, pd.DataFrame]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ
        ì‹¤íŒ¨ì‹œ ë¡œì»¬ CSV íŒŒì¼ì„ ì‚¬ìš©
        """
        start_time = pd.to_datetime(start, utc=True)
        end_time = pd.to_datetime(end, utc=True)
        datasets = {}
        
        try:
            # ìš°ì„  ë¡œì»¬ ë°ì´í„° ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
            raise ValueError('use local data')
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ìš´ì˜ì‹œ ì‚¬ìš©)
            for table_name in prism_core_db.get_tables():
                df = prism_core_db.get_table_data(table_name)
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                    df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                datasets[table_name] = df
                
        except Exception as e:
            print(f"dataset error raised {e}, use local data")
            # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            from glob import glob
            import os
            
            data_paths = glob('prism_monitor/data/Industrial_DB_sample/*.csv')
            for data_path in data_paths:
                df = pd.read_csv(data_path)
                table_name = os.path.basename(data_path).split('.csv')[0].lower()
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                    # df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                    print(f"ì‹œê°„ í•„í„°ë§ ë¹„í™œì„±í™” - ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(df)}í–‰")
                datasets[table_name] = df
                
        return datasets
    
    def _detect_anomalies_in_data(self, data: pd.DataFrame, table_name: str) -> List[Dict]:
        """ë°ì´í„°ì—ì„œ ì´ìƒ ê°ì§€"""
        anomalies = []
        
        try:
            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) == 0:
                return anomalies
            
            # Isolation Forestë¥¼ ì‚¬ìš©í•œ ì´ìƒíƒì§€
            if len(data) >= 10:  # ìµœì†Œ ë°ì´í„° ìˆ˜ í™•ì¸
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                outlier_labels = iso_forest.fit_predict(data[numeric_cols].fillna(0))
                
                # ì´ìƒì¹˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                anomaly_indices = np.where(outlier_labels == -1)[0]
                
                for idx in anomaly_indices:
                    anomaly_record = {
                        'table_name': table_name,
                        'timestamp': data.iloc[idx].get('timestamp', datetime.now().isoformat()),
                        'equipment_id': data.iloc[idx].get('equipment_id', 'unknown'),
                        'anomaly_type': 'statistical_outlier',
                        'severity': 'MEDIUM',
                        'anomaly_score': abs(iso_forest.score_samples(data[numeric_cols].iloc[[idx]].fillna(0))[0]),
                        'affected_parameters': [],
                        'detection_method': 'isolation_forest'
                    }
                    
                    # ì´ìƒ íŒŒë¼ë¯¸í„° ì‹ë³„
                    for col in numeric_cols:
                        value = data.iloc[idx][col]
                        if pd.notna(value):
                            col_mean = data[col].mean()
                            col_std = data[col].std()
                            if col_std > 0:
                                z_score = abs((value - col_mean) / col_std)
                                if z_score > 2:  # 2-sigma ì´ìƒ
                                    anomaly_record['affected_parameters'].append({
                                        'parameter': col,
                                        'value': float(value),
                                        'z_score': float(z_score),
                                        'mean': float(col_mean),
                                        'std': float(col_std)
                                    })
                    
                    if anomaly_record['affected_parameters']:
                        anomalies.append(anomaly_record)
                        
        except Exception as e:
            print(f"{table_name} ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return anomalies
    
    def _create_anomaly_visualization(self, anomalies: List[Dict], all_data: Dict[str, pd.DataFrame]) -> str:
        """ì´ìƒ í˜„ìƒ ì‹œê°í™” ìƒì„±"""
        if not anomalies and not all_data:
            return self._create_empty_svg()
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. í…Œì´ë¸”ë³„ ì´ìƒ ê°œìˆ˜
            if anomalies:
                table_counts = {}
                for anomaly in anomalies:
                    table_name = anomaly.get('table_name', 'unknown')
                    table_counts[table_name] = table_counts.get(table_name, 0) + 1
                
                if table_counts:
                    axes[0, 0].bar(table_counts.keys(), table_counts.values(), color='red', alpha=0.7)
                    axes[0, 0].set_title('Anomalies by Table')
                    axes[0, 0].set_ylabel('Anomaly Count')
                    axes[0, 0].tick_params(axis='x', rotation=45)
                else:
                    axes[0, 0].text(0.5, 0.5, 'No anomalies detected', ha='center', va='center', transform=axes[0, 0].transAxes)
            else:
                axes[0, 0].text(0.5, 0.5, 'No anomalies detected', ha='center', va='center', transform=axes[0, 0].transAxes)
            axes[0, 0].set_title('Anomalies by Table')
            
            # 2. ì‹¬ê°ë„ë³„ ë¶„í¬
            if anomalies:
                severity_counts = {}
                for anomaly in anomalies:
                    severity = anomaly.get('severity', 'MEDIUM')
                    severity_counts[severity] = severity_counts.get(severity, 0) + 1
                
                if severity_counts:
                    colors = ['red' if s == 'HIGH' else 'orange' if s == 'MEDIUM' else 'yellow' for s in severity_counts.keys()]
                    axes[0, 1].pie(severity_counts.values(), labels=severity_counts.keys(), 
                                  autopct='%1.1f%%', colors=colors, startangle=90)
                else:
                    axes[0, 1].text(0.5, 0.5, 'No severity data', ha='center', va='center', transform=axes[0, 1].transAxes)
            else:
                axes[0, 1].text(0.5, 0.5, 'No severity data', ha='center', va='center', transform=axes[0, 1].transAxes)
            axes[0, 1].set_title('Severity Distribution')
            
            # 3. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (í…Œì´ë¸”ë³„)
            if all_data:
                quality_scores = {}
                for table_name, data in all_data.items():
                    validation_result = self.data_validator.validate_data_integrity(data, table_name)
                    quality_scores[table_name] = validation_result.get('data_quality_score', 0)
                
                if quality_scores:
                    colors = ['green' if score > 80 else 'yellow' if score > 60 else 'red' for score in quality_scores.values()]
                    axes[1, 0].bar(quality_scores.keys(), quality_scores.values(), color=colors, alpha=0.7)
                    axes[1, 0].set_title('Data Quality Scores')
                    axes[1, 0].set_ylabel('Quality Score (%)')
                    axes[1, 0].set_ylim(0, 100)
                    axes[1, 0].tick_params(axis='x', rotation=45)
                else:
                    axes[1, 0].text(0.5, 0.5, 'No quality data', ha='center', va='center', transform=axes[1, 0].transAxes)
            else:
                axes[1, 0].text(0.5, 0.5, 'No quality data', ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('Data Quality Scores')
            
            # 4. ì‹œê°„ë³„ ì´ìƒ ë°œìƒ ì¶”ì´
            if anomalies:
                try:
                    anomaly_times = []
                    for anomaly in anomalies:
                        timestamp = anomaly.get('timestamp')
                        if timestamp:
                            try:
                                anomaly_times.append(pd.to_datetime(timestamp))
                            except:
                                anomaly_times.append(datetime.now())
                    
                    if anomaly_times:
                        time_df = pd.DataFrame({'timestamp': anomaly_times})
                        time_df['hour'] = time_df['timestamp'].dt.floor('H')
                        time_counts = time_df.groupby('hour').size()
                        
                        if len(time_counts) > 0:
                            axes[1, 1].plot(time_counts.index, time_counts.values, 
                                           marker='o', color='red', linewidth=2)
                            axes[1, 1].fill_between(time_counts.index, time_counts.values, alpha=0.3, color='red')
                            axes[1, 1].set_xlabel('Time')
                            axes[1, 1].set_ylabel('Anomaly Count')
                            axes[1, 1].tick_params(axis='x', rotation=45)
                        else:
                            axes[1, 1].text(0.5, 0.5, 'No time data', ha='center', va='center', transform=axes[1, 1].transAxes)
                    else:
                        axes[1, 1].text(0.5, 0.5, 'No time data', ha='center', va='center', transform=axes[1, 1].transAxes)
                except Exception as e:
                    axes[1, 1].text(0.5, 0.5, f'Time chart error: {str(e)[:30]}', ha='center', va='center', transform=axes[1, 1].transAxes)
            else:
                axes[1, 1].text(0.5, 0.5, 'No anomaly timeline', ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('Anomaly Timeline')
            
            plt.tight_layout()
            
            # SVG ìƒì„±
            svg_buffer = io.StringIO()
            plt.savefig(svg_buffer, format='svg', bbox_inches='tight', 
                       facecolor='white', edgecolor='none', dpi=100)
            svg_content = svg_buffer.getvalue()
            svg_buffer.close()
            plt.close(fig)
            
            return svg_content
            
        except Exception as e:
            print(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_svg(str(e))
    
    def _create_empty_svg(self) -> str:
        """ë¹ˆ ê²°ê³¼ìš© SVG ìƒì„±"""
        return '''
        <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
            <rect width="800" height="400" fill="white" stroke="black" stroke-width="1"/>
            <text x="400" y="200" text-anchor="middle" font-size="18" fill="green">
                ì´ìƒì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            </text>
            <text x="400" y="230" text-anchor="middle" font-size="14" fill="gray">
                ëª¨ë“  ì‹œìŠ¤í…œì´ ì •ìƒ ìƒíƒœì…ë‹ˆë‹¤.
            </text>
        </svg>
        '''.strip()
    
    def _create_empty_drift_svg(self) -> str:
        """ë¹ˆ ë“œë¦¬í”„íŠ¸ ê²°ê³¼ìš© SVG ìƒì„±"""
        return '''
        <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
            <rect width="800" height="400" fill="white" stroke="black" stroke-width="1"/>
            <text x="400" y="200" text-anchor="middle" font-size="18" fill="green">
                ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            </text>
            <text x="400" y="230" text-anchor="middle" font-size="14" fill="gray">
                ëª¨ë“  ì¥ë¹„ê°€ ì •ìƒ ìƒíƒœë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
            </text>
        </svg>
        '''.strip()
    
    def _create_error_svg(self, error_message: str) -> str:
        """ì˜¤ë¥˜ìš© SVG ìƒì„±"""
        return f'''
        <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
            <rect width="800" height="400" fill="white" stroke="black" stroke-width="1"/>
            <text x="400" y="180" text-anchor="middle" font-size="16" fill="red">
                ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
            </text>
            <text x="400" y="210" text-anchor="middle" font-size="12" fill="gray">
                ì˜¤ë¥˜: {error_message[:50]}...
            </text>
            <text x="400" y="240" text-anchor="middle" font-size="12" fill="blue">
                ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
            </text>
        </svg>
        '''.strip()

# ============================================================================
# ğŸ†• NEW VERSION: File-Based Model Support (CSV íŒŒì¼ë³„ ëª¨ë¸ ì§€ì› - 20ê°œ ëª¨ë¸)
# ============================================================================
def detect_anomalies_realtime(prism_core_db, start: str, end: str,
                               target_file: str = None,  # ğŸ†• ìˆ˜ì •: CSV íŒŒì¼ ì‹ë³„ì ì§€ì •
                               target_process: str = None,  # ğŸ“ DEPRECATED: í•˜ìœ„ í˜¸í™˜ìš©
                               model_dir: str = "models",
                               use_csv: bool = False):  # ğŸ†• CSV íŒŒì¼ ì§ì ‘ ì½ê¸° ì˜µì…˜
    """
    ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ì´ìƒíƒì§€ ìˆ˜í–‰

    Args:
        prism_core_db: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (use_csv=Trueë©´ None ê°€ëŠ¥)
        start: ì‹œì‘ ì‹œê°„ (ISO format)
        end: ì¢…ë£Œ ì‹œê°„ (ISO format)
        target_file: ğŸ†• íƒì§€í•  CSV íŒŒì¼ (ì˜ˆ: 'semiconductor_cmp_001', 'automotive_welding_001')
                    Noneì´ë©´ ë ˆê±°ì‹œ ëª¨ë“œ
        target_process: ğŸ“ DEPRECATED - í•˜ìœ„ í˜¸í™˜ìš©, target_file ì‚¬ìš© ê¶Œì¥
        model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: "models")
        use_csv: ë¡œì»¬ CSV íŒŒì¼ ì§ì ‘ ì½ê¸° (API ëŒ€ì‹ )

    Returns:
        (anomalies, drift_results, analysis_summary, vis_json)
    """
    print(f"ğŸ†• ì‹¤ì‹œê°„ ì´ìƒíƒì§€ ì‹œì‘ (File-Based Model Mode): {start} ~ {end}")

    # í•˜ìœ„ í˜¸í™˜: target_processê°€ ìˆìœ¼ë©´ target_fileë¡œ ë³€í™˜
    if not target_file and target_process:
        print(f"   âš ï¸  target_processëŠ” deprecatedë©ë‹ˆë‹¤. target_fileì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        target_file = target_process

    if target_file:
        print(f"   ëŒ€ìƒ íŒŒì¼: {target_file}")

    # ğŸ†• ê³µì •ë³„ ëª¨ë¸ ì§€ì›
    if target_file or target_process:
        return _detect_with_process_specific_model(
            prism_core_db, start, end, target_file or target_process, model_dir, use_csv=use_csv
        )
    else:
        # targetì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬
        raise ValueError("target_file ë˜ëŠ” target_processë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")


# ============================================================================
# ğŸ†• HELPER FUNCTIONS: JSON ì§ë ¬í™”
# ============================================================================
def convert_to_json_serializable(obj):
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    import pandas as pd
    import numpy as np
    from datetime import datetime, date

    # numpy ë°°ì—´ì´ë‚˜ pandas Series/DataFrame ì²˜ë¦¬
    if isinstance(obj, (np.ndarray, pd.Series)):
        return [convert_to_json_serializable(item) for item in obj.tolist()]
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(orient="records")

    # scalar ê°’ë“¤ì— ëŒ€í•œ NA ì²´í¬ (ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
    try:
        if pd.isna(obj):
            return None
    except (ValueError, TypeError):
        # ë°°ì—´ì´ê±°ë‚˜ NA ì²´í¬ê°€ ë¶ˆê°€ëŠ¥í•œ ê°ì²´ëŠ” ê·¸ëƒ¥ ë„˜ì–´ê°
        pass

    if isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        if np.isnan(obj):
            return None
        return float(obj)
    elif isinstance(obj, (datetime, date)):
        return obj.isoformat()
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, bytes):
        return obj.decode('utf-8', errors='ignore')
    elif isinstance(obj, dict):
        return {k: convert_to_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_to_json_serializable(item) for item in obj]
    else:
        return obj


def dataframe_to_json_serializable(df):
    """DataFrameì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
    if df.empty:
        return []

    # ê° í–‰ì„ dictë¡œ ë³€í™˜í•˜ë©´ì„œ ëª¨ë“  ê°’ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
    records = []
    for _, row in df.iterrows():
        record = {}
        for col in df.columns:
            val = row[col]
            record[col] = convert_to_json_serializable(val)
        records.append(record)

    return records


# ============================================================================
# ğŸ†• HELPER FUNCTION: CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
# ============================================================================
def _load_data_from_csv(target_process: str, start: str, end: str):
    """
    ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ

    Args:
        target_process: ê³µì • ì‹ë³„ì
        start: ì‹œì‘ ì‹œê°„
        end: ì¢…ë£Œ ì‹œê°„

    Returns:
        pandas DataFrame
    """
    import pandas as pd

    # ê³µì • ì´ë¦„ -> CSV íŒŒì¼ ê²½ë¡œ ë§¤í•‘
    process_to_csv = {
        # Semiconductor
        'semiconductor_cmp_001': 'prism_monitor/test-scenarios/test_data/semiconductor/semiconductor_cmp_001.csv',
        'semiconductor_etch_002': 'prism_monitor/test-scenarios/test_data/semiconductor/semiconductor_etch_002.csv',
        'semiconductor_deposition_003': 'prism_monitor/test-scenarios/test_data/semiconductor/semiconductor_deposition_003.csv',
        'semiconductor_full_004': 'prism_monitor/test-scenarios/test_data/semiconductor/semiconductor_full_004.csv',
        # Chemical
        'chemical_reactor_001': 'prism_monitor/test-scenarios/test_data/chemical/chemical_reactor_001.csv',
        'chemical_distillation_002': 'prism_monitor/test-scenarios/test_data/chemical/chemical_distillation_002.csv',
        'chemical_refining_003': 'prism_monitor/test-scenarios/test_data/chemical/chemical_refining_003.csv',
        'chemical_full_004': 'prism_monitor/test-scenarios/test_data/chemical/chemical_full_004.csv',
        # Automotive
        'automotive_welding_001': 'prism_monitor/test-scenarios/test_data/automotive/automotive_welding_001.csv',
        'automotive_painting_002': 'prism_monitor/test-scenarios/test_data/automotive/automotive_painting_002.csv',
        'automotive_press_003': 'prism_monitor/test-scenarios/test_data/automotive/automotive_press_003.csv',
        'automotive_assembly_004': 'prism_monitor/test-scenarios/test_data/automotive/automotive_assembly_004.csv',
        # Battery
        'battery_formation_001': 'prism_monitor/test-scenarios/test_data/battery/battery_formation_001.csv',
        'battery_coating_002': 'prism_monitor/test-scenarios/test_data/battery/battery_coating_002.csv',
        'battery_aging_003': 'prism_monitor/test-scenarios/test_data/battery/battery_aging_003.csv',
        'battery_production_004': 'prism_monitor/test-scenarios/test_data/battery/battery_production_004.csv',
        # Steel
        'steel_rolling_001': 'prism_monitor/test-scenarios/test_data/steel/steel_rolling_001.csv',
        'steel_converter_002': 'prism_monitor/test-scenarios/test_data/steel/steel_converter_002.csv',
        'steel_casting_003': 'prism_monitor/test-scenarios/test_data/steel/steel_casting_003.csv',
        'steel_production_004': 'prism_monitor/test-scenarios/test_data/steel/steel_production_004.csv',
    }

    # CSV íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    csv_path = process_to_csv.get(target_process)
    if not csv_path:
        # ëŒ€ë¬¸ìë¡œ ì‹œë„
        csv_path = process_to_csv.get(target_process.upper())

    if not csv_path or not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_process}")

    print(f"   ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")

    # CSV íŒŒì¼ ì½ê¸°
    data = pd.read_csv(csv_path)
    print(f"   âœ“ ë¡œë“œ ì™„ë£Œ: {len(data)} í–‰")

    # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
    data.columns = data.columns.str.lower()

    # Timestamp í•„í„°ë§
    if 'timestamp' in data.columns:
        data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True)
        start_time = pd.to_datetime(start, utc=True)
        end_time = pd.to_datetime(end, utc=True)
        data = data[(data['timestamp'] >= start_time) & (data['timestamp'] <= end_time)]
        print(f"   âœ“ ì‹œê°„ í•„í„°ë§ ì™„ë£Œ: {len(data)} í–‰ (ì‹œê°„ ë²”ìœ„: {start} ~ {end})")

    return data


# ============================================================================
# ğŸ†• NEW FUNCTION: Process-Specific Model Detection (ê³µì •ë³„ ëª¨ë¸)
# ============================================================================
def _detect_with_process_specific_model(prism_core_db, start: str, end: str,
                                        target_process: str, model_dir: str, use_csv: bool = False):
    """
    ê³µì •ë³„ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ìƒ íƒì§€ (API ë˜ëŠ” CSV ê¸°ë°˜ ë°ì´í„° ë¡œë”©)

    Args:
        target_process: ê³µì • ì‹ë³„ì (ì˜ˆ: 'semi_cmp_sensors', 'semiconductor_cmp_001')
        use_csv: Trueì´ë©´ ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    """
    from prism_monitor.utils.process_model_manager import ProcessModelManager
    import pandas as pd

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    target_file = target_process

    print(f"ğŸ” ê³µì •ë³„ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€ ìˆ˜í–‰: {target_process}")
    if use_csv:
        print(f"   ğŸ“ ë°ì´í„° ì†ŒìŠ¤: ë¡œì»¬ CSV íŒŒì¼ (ê°•ì œ)")
    else:
        print(f"   ğŸŒ ë°ì´í„° ì†ŒìŠ¤: API ìš°ì„ , ì‹¤íŒ¨ ì‹œ CSV í´ë°±")

    try:
        # 1. ProcessModelManager ì´ˆê¸°í™”
        process_model_manager = ProcessModelManager(base_model_dir=model_dir)

        # 2. ëª¨ë¸ ë¡œë“œ
        try:
            model, scaler, metadata = process_model_manager.get_model_for_process(target_process)
            feature_cols = metadata['feature_columns']
            threshold = metadata['threshold']
            print(f"   âœ“ {target_process} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (features: {len(feature_cols)})")
        except Exception as e:
            print(f"   âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({target_process}): {e}")
            return [], [], {'error': f'Model not found: {e}'}, {"error": str(e)}

        # 3. ë°ì´í„° ë¡œë”© (API ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ CSVë¡œ í´ë°±)
        data = None
        data_source = None

        if use_csv:
            # ëª…ì‹œì ìœ¼ë¡œ CSV ëª¨ë“œ ì§€ì •ëœ ê²½ìš°
            print(f"   ğŸ“ CSV ëª¨ë“œ ê°•ì œ ì‚¬ìš©")
            try:
                data = _load_data_from_csv(target_process, start, end)
                data_source = "CSV (ê°•ì œ)"
            except Exception as e:
                print(f"   âœ— CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
                return [], [], {'error': f'CSV loading failed: {e}'}, {"error": str(e)}
        else:
            # API ë¨¼ì € ì‹œë„
            if prism_core_db is not None:
                table_name = target_process.upper() if not target_process.startswith('semiconductor_') else target_process
                print(f"   ğŸ“‚ APIì—ì„œ ë°ì´í„° ë¡œë“œ ì‹œë„: {table_name}")
                try:
                    data = prism_core_db.get_table_data(table_name)
                    print(f"   âœ“ API ë¡œë“œ ì™„ë£Œ: {len(data)} í–‰")
                    data_source = "API"

                    # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                    data.columns = data.columns.str.lower()

                    # Timestamp í•„í„°ë§
                    if 'timestamp' in data.columns:
                        data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True)
                        start_time = pd.to_datetime(start, utc=True)
                        end_time = pd.to_datetime(end, utc=True)
                        data = data[(data['timestamp'] >= start_time) & (data['timestamp'] <= end_time)]
                        print(f"   âœ“ ì‹œê°„ í•„í„°ë§ ì™„ë£Œ: {len(data)} í–‰ (ì‹œê°„ ë²”ìœ„: {start} ~ {end})")

                except Exception as e:
                    print(f"   âš ï¸  API ë¡œë“œ ì‹¤íŒ¨: {e}")
                    print(f"   ğŸ”„ ë¡œì»¬ CSV íŒŒì¼ë¡œ í´ë°±...")
                    data = None

            # API ì‹¤íŒ¨ ë˜ëŠ” prism_core_dbê°€ Noneì¸ ê²½ìš° CSVë¡œ í´ë°±
            if data is None:
                try:
                    data = _load_data_from_csv(target_process, start, end)
                    data_source = "CSV (í´ë°±)"
                except Exception as e:
                    print(f"   âœ— CSV ë¡œë“œë„ ì‹¤íŒ¨: {e}")
                    return [], [], {'error': f'Both API and CSV loading failed: {e}'}, {"error": str(e)}

        if len(data) == 0:
            print(f"   âš ï¸  ì‹œê°„ ë²”ìœ„ ë‚´ ë°ì´í„° ì—†ìŒ")
            return [], [], {}, {"anomalies": [], "drift_results": [], "raw_data": {}}

        # 4. Feature ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        # ë¬¸ìì—´ ì»¬ëŸ¼ ìë™ í•„í„°ë§ ë° ëˆ„ë½ëœ feature ì²˜ë¦¬
        numeric_feature_cols = []
        for col in feature_cols:
            if col not in data.columns:
                # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                data[col] = 0
                numeric_feature_cols.append(col)
            else:
                # ìˆ«ìí˜• ë°ì´í„°ë§Œ í¬í•¨
                if pd.api.types.is_numeric_dtype(data[col]):
                    numeric_feature_cols.append(col)
                else:
                    print(f"   âš ï¸  ë¹„ìˆ«ì ì»¬ëŸ¼ ì œì™¸: {col} (íƒ€ì…: {data[col].dtype})")

        # ìˆ«ìí˜• featureë§Œ ì‚¬ìš©
        if len(numeric_feature_cols) != len(feature_cols):
            print(f"   ğŸ“ Feature ì¡°ì •: {len(feature_cols)} â†’ {len(numeric_feature_cols)} (ìˆ«ìí˜•ë§Œ)")
            feature_cols = numeric_feature_cols

        X_test = data[feature_cols].values
        X_test = np.nan_to_num(X_test, nan=0.0)
        X_test_scaled = scaler.transform(X_test)

        # 5. ì´ìƒíƒì§€
        reconstructed = model.predict(X_test_scaled, verbose=0)
        mse_scores = np.mean(np.square(X_test_scaled - reconstructed), axis=1)

        # 6. ì´ìƒì¹˜ íŒì •
        anomaly_mask = mse_scores > threshold
        anomaly_indices = np.where(anomaly_mask)[0]

        print(f"   ğŸ“Š {len(anomaly_indices)}ê°œ ì´ìƒì¹˜ íƒì§€ (ì „ì²´ {len(data)}ê°œ ì¤‘)")

        # 7. ì´ìƒì¹˜ ë ˆì½”ë“œ ìƒì„±
        anomalies = []
        for idx in anomaly_indices:
            anomaly_record = {
                'table_name': target_file,
                'file_identifier': target_file,
                'timestamp': data.iloc[idx].get('timestamp', datetime.now()).isoformat() if hasattr(data.iloc[idx].get('timestamp'), 'isoformat') else str(data.iloc[idx].get('timestamp')),
                'equipment_id': data.iloc[idx].get('sensor_id') or data.iloc[idx].get('equipment_id', 'unknown'),
                'anomaly_type': 'autoencoder_reconstruction_error',
                'anomaly_score': float(mse_scores[idx]),
                'threshold': float(threshold),
                'severity': 'HIGH' if mse_scores[idx] > threshold * 2 else 'MEDIUM',
                'model_used': metadata['model_version'],
                'detection_method': 'file_specific_autoencoder'
            }
            anomalies.append(anomaly_record)

        # 8. ë¶„ì„ ìš”ì•½
        analysis_summary = {
            'total_records': len(data),
            'anomalies_detected': len(anomalies),
            'target_file': target_file,
            'processing_mode': 'process_specific_model',
            'data_source': data_source,  # API ë˜ëŠ” CSV
            'processing_time': datetime.now().isoformat(),
            'model_version': metadata['model_version'],
            'threshold': float(threshold)
        }

        # 9. vis_json ìƒì„±
        vis_json = {
            "anomalies": convert_to_json_serializable(anomalies),
            "drift_results": [],
            "raw_data": {target_file: dataframe_to_json_serializable(data)},
            "analysis_summary": analysis_summary
        }

        print(f"âœ… íŒŒì¼ë³„ ì´ìƒíƒì§€ ì™„ë£Œ: {len(anomalies)}ê°œ ì´ìƒ íƒì§€")
        return anomalies, [], analysis_summary, vis_json

    except Exception as e:
        print(f"âŒ íŒŒì¼ë³„ ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return [], [], {'error': str(e)}, {"error": str(e)}


# ============================================================================
# ğŸ“ LEGACY VERSION: ê¸°ì¡´ ë°©ì‹ (ëª¨ë“  ë°ì´í„° í†µí•© íƒì§€)
# ============================================================================
def _detect_anomalies_realtime_legacy(prism_core_db, start: str, end: str, model_dir: str = "models"):
    """
    ê¸°ì¡´ ë°©ì‹: ëª¨ë“  ì„¼ì„œ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€
    (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ë¥¼ ìœ„í•´ ë³´ì¡´)
    """
    print(f"ğŸ“ Legacy Mode: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì´ìƒíƒì§€ ìˆ˜í–‰: {start} ~ {end}")
    
    def convert_to_json_serializable(obj):
        """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        import pandas as pd
        import numpy as np
        from datetime import datetime, date
        
        # numpy ë°°ì—´ì´ë‚˜ pandas Series/DataFrame ì²˜ë¦¬
        if isinstance(obj, (np.ndarray, pd.Series)):
            return [convert_to_json_serializable(item) for item in obj.tolist()]
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict(orient="records")
        
        # scalar ê°’ë“¤ì— ëŒ€í•œ NA ì²´í¬ (ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
        try:
            if pd.isna(obj):
                return None
        except (ValueError, TypeError):
            # ë°°ì—´ì´ê±°ë‚˜ NA ì²´í¬ê°€ ë¶ˆê°€ëŠ¥í•œ ê°ì²´ëŠ” ê·¸ëƒ¥ ë„˜ì–´ê°
            pass
        
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            if np.isnan(obj):
                return None
            return float(obj)
        elif isinstance(obj, (datetime, date)):
            return obj.isoformat()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, bytes):
            return obj.decode('utf-8', errors='ignore')
        elif isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_json_serializable(item) for item in obj]
        else:
            return obj

    def dataframe_to_json_serializable(df):
        """DataFrameì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
        if df.empty:
            return []
        
        # DataFrameì„ dictë¡œ ë³€í™˜
        records = df.to_dict(orient="records")
        
        # ê° ë ˆì½”ë“œë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
        json_records = []
        for record in records:
            json_record = convert_to_json_serializable(record)
            json_records.append(json_record)
        
        return json_records
    
    try:
        # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        all_data = _fetch_data_from_database_standalone(prism_core_db, start, end)
        
        if not all_data:
            vis_json = {
                "anomalies": [],
                "drift_results": [],
                "raw_data": {}
            }
            return [], [], {}, vis_json
        
        # 2. ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        monitor = EnhancedSemiconductorRealTimeMonitor(model_dir=model_dir)
        
        # 3. ì´ìƒíƒì§€ ìˆ˜í–‰
        anomalies = []
        drift_results = []
        analysis_summary = {
            'total_records': 0,
            'tables_processed': 0,
            'anomalies_detected': 0,
            'drift_detected': 0,
            'processing_time': datetime.now().isoformat()
        }
        
        for table_name, data in all_data.items():
            print(f"ì²˜ë¦¬ ì¤‘ì¸ í…Œì´ë¸”: {table_name}, ë°ì´í„° ìˆ˜: {len(data)}")
            
            if data.empty:
                continue
            
            analysis_summary['total_records'] += len(data)
            analysis_summary['tables_processed'] += 1
            
            # ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬ (monitorê°€ data_validatorë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´)
            try:
                if hasattr(monitor, 'data_validator'):
                    validated_data = monitor.data_validator.preprocess_and_clean(data, table_name)
                else:
                    validated_data = data  # fallback
            except Exception as e:
                print(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                validated_data = data  # fallback
            
            # ì´ìƒíƒì§€ ìˆ˜í–‰ (monitorì˜ ë©”ì„œë“œ ì‚¬ìš©)
            try:
                if hasattr(monitor, '_detect_anomalies_in_data'):
                    table_anomalies = monitor._detect_anomalies_in_data(validated_data, table_name)
                elif hasattr(monitor, 'detect_anomalies_in_data'):
                    table_anomalies = monitor.detect_anomalies_in_data(validated_data, table_name)
                else:
                    # ê¸°ë³¸ ì´ìƒíƒì§€ ë¡œì§ êµ¬í˜„
                    table_anomalies = _basic_anomaly_detection(validated_data, table_name)
                
                anomalies.extend(table_anomalies)
            except Exception as e:
                print(f"ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
            
            # ë“œë¦¬í”„íŠ¸ ê°ì§€ (ì¥ë¹„ë³„ë¡œ ìˆ˜í–‰)
            try:
                if 'equipment_id' in data.columns and hasattr(monitor, 'normal_state_manager'):
                    for equipment_id in data['equipment_id'].unique():
                        equipment_data = data[data['equipment_id'] == equipment_id]
                        drift_result = monitor.normal_state_manager.detect_profile_drift(
                            equipment_id, table_name, equipment_data
                        )
                        if drift_result and drift_result.get('drift_detected'):
                            drift_results.append(drift_result)
            except Exception as e:
                print(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        analysis_summary['anomalies_detected'] = len(anomalies)
        analysis_summary['drift_detected'] = len(drift_results)
        
        # 4. vis_json ìƒì„± (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
        vis_json = {
            "anomalies": [],
            "drift_results": [],
            "raw_data": {}
        }
        
        try:
            # anomalies ì•ˆì „í•˜ê²Œ ë³€í™˜
            print("anomalies ë³€í™˜ ì¤‘...")
            vis_json["anomalies"] = convert_to_json_serializable(anomalies)
        except Exception as e:
            print(f"anomalies ë³€í™˜ ì˜¤ë¥˜: {e}")
            vis_json["anomalies"] = []
            
        try:
            # drift_results ì•ˆì „í•˜ê²Œ ë³€í™˜
            print("drift_results ë³€í™˜ ì¤‘...")
            vis_json["drift_results"] = convert_to_json_serializable(drift_results)
        except Exception as e:
            print(f"drift_results ë³€í™˜ ì˜¤ë¥˜: {e}")
            vis_json["drift_results"] = []
            
        try:
            # raw_dataë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
            print("raw_data ë³€í™˜ ì¤‘...")
            for tbl, df in all_data.items():
                print(f"  í…Œì´ë¸” {tbl} ë³€í™˜ ì¤‘...")
                vis_json["raw_data"][tbl] = dataframe_to_json_serializable(df)
        except Exception as e:
            print(f"raw_data ë³€í™˜ ì˜¤ë¥˜: {e}")
            vis_json["raw_data"] = {}
            
        # ìµœì¢… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            import json
            json.dumps(vis_json)
            print("vis_json JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        except Exception as e:
            print(f"vis_json JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            vis_json = {
                "anomalies": [],
                "drift_results": [],
                "raw_data": {},
                "serialization_error": str(e)
            }
        
        print(f"ì´ìƒíƒì§€ ì™„ë£Œ: ì´ìƒ {len(anomalies)}ê°œ, ë“œë¦¬í”„íŠ¸ {len(drift_results)}ê°œ")
        
        return anomalies, drift_results, analysis_summary, vis_json
        
    except Exception as e:
        print(f"ì‹¤ì‹œê°„ ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        error_analysis = {
            'error': str(e),
            'processing_time': datetime.now().isoformat(),
            'status': 'error'
        }
        vis_json = {
            "anomalies": [],
            "drift_results": [],
            "raw_data": {},
            "error": str(e)
        }
        return [], [], error_analysis, vis_json

# ============================================================================
# ğŸ†• NEW VERSION: ìƒˆ ë°ì´í„° ê²½ë¡œ ë° ì •ê·œí™” ì§€ì›
# ============================================================================
def _fetch_data_from_database_standalone(prism_core_db, start: str, end: str, target_process: str = None):
    """
    ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ (ìƒˆ ë°ì´í„° ê²½ë¡œ ë° ê³µì •ë³„ í•„í„°ë§ ì§€ì›)

    Args:
        prism_core_db: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        start: ì‹œì‘ ì‹œê°„
        end: ì¢…ë£Œ ì‹œê°„
        target_process: ğŸ†• íŠ¹ì • ê³µì •ë§Œ ë¡œë“œ (ì˜ˆ: 'semi_cmp_sensors')
                       Noneì´ë©´ ëª¨ë“  ê³µì • ë¡œë“œ
    """
    import pandas as pd
    from glob import glob
    import os

    # ğŸ†• ë°ì´í„° ì •ê·œí™” import
    try:
        from prism_monitor.utils.data_normalizer import normalize_semiconductor_data, map_file_to_table_name
        use_normalizer = True
    except ImportError:
        print("Warning: data_normalizer not found, using legacy mode")
        use_normalizer = False

    start_time = pd.to_datetime(start, utc=True)
    end_time = pd.to_datetime(end, utc=True)
    datasets = {}

    # ğŸ†• íŒŒì¼ëª…ê³¼ ê³µì • ë§¤í•‘
    file_to_process_map = {
        'semiconductor_cmp_001.csv': 'semi_cmp_sensors',
        'semiconductor_etch_002.csv': 'semi_etch_sensors',
        'semiconductor_deposition_003.csv': 'semi_cvd_sensors',
        # semiconductor_full_004.csvëŠ” ì—¬ëŸ¬ ê³µì • í˜¼í•©ì´ë¯€ë¡œ ì œì™¸ ë˜ëŠ” ë³„ë„ ì²˜ë¦¬
    }

    try:
        # ìš°ì„  ë¡œì»¬ ë°ì´í„° ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
        raise ValueError('use local data')

        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ìš´ì˜ì‹œ ì‚¬ìš©)
        if hasattr(prism_core_db, 'get_tables'):
            for table_name in prism_core_db.get_tables():
                df = prism_core_db.get_table_data(table_name)
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                    df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                datasets[table_name] = df

    except Exception as e:
        print(f"dataset error raised {e}, use local data")

        # ğŸ†• ìƒˆ ë°ì´í„° ê²½ë¡œ ì‹œë„
        try:
            data_paths = glob('prism_monitor/test-scenarios/test_data/semiconductor/*.csv')

            if not data_paths:
                # fallback to old path
                print("ìƒˆ ë°ì´í„° ê²½ë¡œì— íŒŒì¼ ì—†ìŒ, ê¸°ì¡´ ê²½ë¡œ ì‹œë„...")
                data_paths = glob('prism_monitor/data/Industrial_DB_sample/*.csv')
                use_normalizer = False  # ê¸°ì¡´ ë°ì´í„°ëŠ” ì •ê·œí™” ë¶ˆí•„ìš”

            for data_path in data_paths:
                try:
                    source_file = os.path.basename(data_path)

                    # ğŸ†• ìƒˆ ë°ì´í„° ê²½ë¡œì¸ ê²½ìš° íŒŒì¼ëª…ìœ¼ë¡œ ê³µì •ëª… ë§¤í•‘
                    if use_normalizer and source_file in file_to_process_map:
                        table_name = file_to_process_map[source_file]

                        # target_processê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ê³µì •ë§Œ ë¡œë“œ
                        if target_process and table_name != target_process:
                            print(f"   ê³µì • í•„í„°ë§: {table_name} ìŠ¤í‚µ (ëŒ€ìƒ: {target_process})")
                            continue
                    else:
                        # ê¸°ì¡´ ë°ì´í„° ê²½ë¡œ
                        table_name = source_file.split('.csv')[0].lower()

                    df = pd.read_csv(data_path)

                    # ğŸ†• ë°ì´í„° ì •ê·œí™” (ìƒˆ ë°ì´í„°ì¸ ê²½ìš°)
                    if use_normalizer and source_file in file_to_process_map:
                        df = normalize_semiconductor_data(df, source_file)

                    # ì‹œê°„ í•„í„°ë§
                    if 'timestamp' in df.columns:
                        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
                        df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]

                    datasets[table_name] = df
                    print(f"   âœ“ ë¡œë“œ ì™„ë£Œ: {table_name} ({len(df)}í–‰)")

                except Exception as file_error:
                    print(f"   âœ— íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {data_path}, ì˜¤ë¥˜: {file_error}")
                    continue

        except Exception as glob_error:
            print(f"ë¡œì»¬ ë°ì´í„° í´ë” ì ‘ê·¼ ì‹¤íŒ¨: {glob_error}")

    return datasets


# ============================================================================
# ğŸ“ LEGACY VERSION (ì£¼ì„ ì²˜ë¦¬ - ì°¸ê³ ìš©)
# ============================================================================
# def _fetch_data_from_database_standalone(prism_core_db, start: str, end: str):
#     """ë…ë¦½ì ì¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ (ê¸°ì¡´ ë²„ì „)"""
#     import pandas as pd
#     from glob import glob
#     import os
#
#     start_time = pd.to_datetime(start, utc=True)
#     end_time = pd.to_datetime(end, utc=True)
#     datasets = {}
#
#     try:
#         # ìš°ì„  ë¡œì»¬ ë°ì´í„° ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
#         raise ValueError('use local data')
#
#         # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ìš´ì˜ì‹œ ì‚¬ìš©)
#         if hasattr(prism_core_db, 'get_tables'):
#             for table_name in prism_core_db.get_tables():
#                 df = prism_core_db.get_table_data(table_name)
#                 if 'timestamp' in df.columns:
#                     df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
#                     df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
#                 datasets[table_name] = df
#
#     except Exception as e:
#         print(f"dataset error raised {e}, use local data")
#         # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
#         try:
#             data_paths = glob('prism_monitor/data/Industrial_DB_sample/*.csv')
#             for data_path in data_paths:
#                 try:
#                     df = pd.read_csv(data_path)
#                     table_name = os.path.basename(data_path).split('.csv')[0].lower()
#                     if 'timestamp' in df.columns:
#                         df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
#                         df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
#                     datasets[table_name] = df
#                     print(f"ë¡œì»¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {table_name} ({len(df)}í–‰)")
#                 except Exception as file_error:
#                     print(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {data_path}, ì˜¤ë¥˜: {file_error}")
#                     continue
#         except Exception as glob_error:
#             print(f"ë¡œì»¬ ë°ì´í„° í´ë” ì ‘ê·¼ ì‹¤íŒ¨: {glob_error}")
#
#     return datasets


def _basic_anomaly_detection(data, table_name):
    """ê¸°ë³¸ ì´ìƒíƒì§€ ë¡œì§ (fallback)"""
    anomalies = []
    
    try:
        # ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì´ìƒíƒì§€
        import pandas as pd
        import numpy as np
        
        for column in data.select_dtypes(include=[np.number]).columns:
            if column == 'timestamp':
                continue
                
            values = data[column].dropna()
            if len(values) == 0:
                continue
                
            mean = values.mean()
            std = values.std()
            
            if std > 0:
                # 3-sigma ê·œì¹™ ì ìš©
                threshold_upper = mean + 3 * std
                threshold_lower = mean - 3 * std
                
                anomaly_indices = values[(values > threshold_upper) | (values < threshold_lower)].index
                
                for idx in anomaly_indices:
                    anomalies.append({
                        'table_name': table_name,
                        'column': column,
                        'value': float(values[idx]),
                        'threshold_upper': float(threshold_upper),
                        'threshold_lower': float(threshold_lower),
                        'anomaly_score': float(abs(values[idx] - mean) / std),
                        'timestamp': data.loc[idx, 'timestamp'] if 'timestamp' in data.columns else None,
                        'equipment_id': data.loc[idx, 'equipment_id'] if 'equipment_id' in data.columns else None,
                    })
    
    except Exception as e:
        print(f"ê¸°ë³¸ ì´ìƒíƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return anomalies