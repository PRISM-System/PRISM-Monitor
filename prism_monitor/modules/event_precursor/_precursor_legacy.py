# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

class LSTMRegressor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMRegressor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

def load_and_preprocess_data(csv_file):
    # ë°ì´í„° ë¡œë“œ
    photo_sensors_data = pd.read_csv(csv_file)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    photo_sensors_data['TIMESTAMP'] = pd.to_datetime(photo_sensors_data['TIMESTAMP'])
    photo_sensors_data = photo_sensors_data.sort_values('TIMESTAMP').set_index('TIMESTAMP')
    
    # PNO, EQUIPMENT_ID ë“± ë¹„ìˆ˜ì¹˜ ë°ì´í„° ì œì™¸
    features = photo_sensors_data.select_dtypes(include=np.number)
    feature_names = features.columns.tolist()
    
    # ë°ì´í„° ì •ê·œí™”
    scaler = MinMaxScaler()
    scaled_features = scaler.fit_transform(features)
    
    return scaled_features, feature_names, scaler

def create_sequences(data, feature_names, target_column, time_steps=5):
    X, y = [], []
    target_idx = feature_names.index(target_column)
    
    for i in range(len(data) - time_steps):
        X.append(data[i:(i + time_steps)])
        y.append(data[i + time_steps, target_idx])
    
    return np.array(X), np.array(y)

def prepare_training_data(X, y, test_size=0.2):
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)
    
    # PyTorch Tensorë¡œ ë³€í™˜
    X_train = torch.from_numpy(X_train).float()
    y_train = torch.from_numpy(y_train).float().view(-1, 1)
    X_test = torch.from_numpy(X_test).float()
    y_test = torch.from_numpy(y_test).float().view(-1, 1)
    
    return X_train, X_test, y_train, y_test

def train_model(model, X_train, y_train, epochs=30, learning_rate=0.001):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    print("--- ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ---")
    for epoch in range(epochs):
        model.train()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
    
    print("--- ëª¨ë¸ í›ˆë ¨ ì¢…ë£Œ ---\n")
    return model, criterion, optimizer

def evaluate_model(model, criterion, X_test, y_test):
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test)
        test_loss = criterion(test_outputs, y_test)
        print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì†ì‹¤ (MSE): {test_loss.item():.4f}')
    
    return test_loss.item()

def predict_anomaly(model, scaled_features, feature_names, target_column, scaler, time_steps=5, anomaly_threshold=1.5):
    print("--- ë¯¸ë˜ ì´ìƒ ì§•í›„ ì˜ˆì¸¡ ì‹œì‘ ---")
    
    # í˜„ì¬ ìƒíƒœ (ë§ˆì§€ë§‰ time_stepsê°œ ë°ì´í„°)
    current_state_scaled = scaled_features[-time_steps:]
    current_state_tensor = torch.from_numpy(current_state_scaled).float().unsqueeze(0)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    model.eval()
    with torch.no_grad():
        predicted_scaled = model(current_state_tensor)
    
    # ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    dummy_array = np.zeros((1, len(feature_names)))
    target_idx = feature_names.index(target_column)
    dummy_array[0, target_idx] = predicted_scaled.item()
    predicted_original = scaler.inverse_transform(dummy_array)[0, target_idx]
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n[ì…ë ¥] í˜„ì¬ ìƒíƒœ (ë§ˆì§€ë§‰ {time_steps}ê°œ ë°ì´í„°)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡")
    print(f"[ì˜ˆì¸¡] ë‹¤ìŒ ì‹œì ì˜ '{target_column}' ê°’: {predicted_original:.4f}")
    
    is_anomaly = predicted_original > anomaly_threshold
    if is_anomaly:
        print(f"[íŒë‹¨] ğŸš¨ ê²½ê³ : ì˜ˆì¸¡ê°’ì´ ì„ê³„ì¹˜({anomaly_threshold})ë¥¼ ì´ˆê³¼í•˜ì—¬ 'ì´ìƒ ì§•í›„'ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.")
    else:
        print(f"[íŒë‹¨] âœ… ì •ìƒ: ì˜ˆì¸¡ê°’ì´ ì„ê³„ì¹˜({anomaly_threshold}) ì´í•˜ë¡œ ì•ˆì •ì ì¸ ìƒíƒœê°€ ì˜ˆìƒë©ë‹ˆë‹¤.")
    
    return predicted_original, is_anomaly