from dotenv import load_dotenv
load_dotenv()

import requests
import pandas as pd
import streamlit as st
import os
import logging
import time

from tinydb import TinyDB, Query
from typing import Union, Literal
from urllib.parse import urljoin

from prism_monitor.data.database import PrismCoreDataBase

# Constants
DATABASE_PATH = "monitor_db.json"
LOCAL_FILE_DIR = 'prism_monitor/data/local'
LLM_URL = os.environ['LLM_URL']
MONITOR_DB = TinyDB(DATABASE_PATH)
PRISM_CORE_DB = PrismCoreDataBase(os.environ['PRISM_CORE_DATABASE_URL'])
BACKEND_URL = 'http://localhost:8000'

# Streamlit Config
st.set_page_config(layout="wide", page_title="PRISM Monitoring Workflow")
st.title("ğŸ” PRISM: Proactive Monitoring Workflow")
st.markdown("This dashboard provides a step-by-step interface for the PRISM industrial monitoring and analysis system.")

# User input
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", "2024ë…„ 1ì›”ë¶€í„° 2ì›”ê¹Œì§€ì˜ ì´ìƒì¹˜ë¥¼ íƒìƒ‰í•´ì¤˜")

# Button to trigger workflow
if st.button("ì…ë ¥"):
    with st.spinner("ğŸ“¡ ì´ìƒì¹˜ íƒìƒ‰ ì¤‘..."):
        # Step 1: Detect Anomalies
        detect_data = {
            "taskId": "TASK_0001",
            "start": "2024-01-01T12:00:00Z",
            "end": "2024-02-01T12:30:00Z"
        }
        detect_res = requests.post(urljoin(BACKEND_URL, '/api/v1/monitoring/event/detect'), json=detect_data).json()['result']

        st.success("1ï¸âƒ£ ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ")
        with st.expander("ğŸ” íƒì§€ ê²°ê³¼ ë³´ê¸°"):
            st.write('**Anomalies:**', str(detect_res['anomalies']))
            st.write(detect_res['svg'], unsafe_allow_html=True)

        Event = Query()
        event_record = MONITOR_DB.table('EventDetectHistory').get(Event.task_id == detect_data['taskId'])['records']
        record_df = pd.DataFrame(event_record)
        st.dataframe(record_df, use_container_width=True)

    with st.spinner("ğŸ§  ì´ìƒì¹˜ ì„¤ëª… ìƒì„± ì¤‘..."):
        # Step 2: Explain Anomalies
        explain_data = {"taskId": "TASK_0001"}
        explain_res = requests.post(urljoin(BACKEND_URL, '/api/v1/monitoring/event/explain'), json=explain_data).json()

        st.success("2ï¸âƒ£ ì´ìƒì¹˜ ì„¤ëª… ìƒì„± ì™„ë£Œ")
        with st.expander("ğŸ’¬ ì´ìƒì¹˜ ì„¤ëª…"):
            st.text_area("ì„¤ëª… ê²°ê³¼", explain_res['explain'], height=200)

    with st.spinner("ğŸ” ë¬¸ì œ ì›ì¸ í›„ë³´êµ° ë¶„ì„ ì¤‘..."):
        # Step 3: Cause Candidates
        cause_data = {"taskId": "TASK_0001"}
        cause_res = requests.post(urljoin(BACKEND_URL, '/api/v1/monitoring/event/cause-candidates'), json=cause_data).json()

        st.success("3ï¸âƒ£ ë¬¸ì œ ì›ì¸ í›„ë³´êµ° ìƒì„± ì™„ë£Œ")
        with st.expander("ğŸ§© ì›ì¸ í›„ë³´êµ°"):
            st.text_area("í›„ë³´êµ°", str(cause_res['causeCandidates']), height=150)

    with st.spinner("ğŸ”® ì´ìƒì§•í›„ ì˜ˆì¸¡ ì¤‘..."):
        # Step 4: Precursor Prediction
        precursor_data = {
            "taskId": "TASK_0001",
            "start": "2024-01-01T12:00:00Z",
            "end": "2024-02-01T12:30:00Z"
        }
        precursor_res = requests.post(urljoin(BACKEND_URL, '/api/v1/monitoring/event/precursor'), json=precursor_data).json()

        st.success("4ï¸âƒ£ ì´ìƒì§•í›„ ì˜ˆì¸¡ ì™„ë£Œ")
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="ì˜ˆì¸¡ê°’", value=precursor_res['summary']['predicted_value'])
        with col2:
            st.metric(label="ì´ìƒ ì—¬ë¶€", value=str(precursor_res['summary']['is_anomaly']))

    with st.spinner("ğŸ›¡ ìœ„í—˜ë„ í‰ê°€ ì¤‘..."):
        # Step 5: Risk Evaluation
        risk_data = {
            "taskId": "TASK_0001",
            "topk": 5
        }
        risk_res = requests.post(urljoin(BACKEND_URL, '/api/v1/monitoring/event/evaluate-risk'), json=risk_data).json()

        st.success("5ï¸âƒ£ ìœ„í—˜ë„ í‰ê°€ ì™„ë£Œ")
        st.subheader("âš  í˜„ì¬ ìƒíƒœ ìœ„í—˜ë„ í‰ê°€")
        st.markdown("**Event Evaluation:**")
        st.write(risk_res['summary']['eventEvaluation'])
        st.markdown("**Prediction Evaluation:**")
        st.write(risk_res['summary']['predictionEvaluation'])
